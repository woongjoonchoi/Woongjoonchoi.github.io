var store = [{
        "title": "Optimization",
        "excerpt":"Optimization   Batch vs Mini-Batch   Gradient Descent   Stochastic Gradient Descent  ","categories": [],
        "tags": [],
        "url": "/Optimization/",
        "teaser": null
      },{
        "title": "Pytorch Tutorial Tensor Tutorial",
        "excerpt":"PyTorch tutorial-tensor tutorial Pytorch의 기본적인 코드들을 고민하지 않고 의식하지 않고 사용하기 위해 반복연습이 중요하다 느꼈습니다. 따라서, torch 튜토리얼을 진행하게 되었습니다.Pytorch 공식 튜토리얼을 진행하면서 추가적으로 알게된 사실들을 적습니다 . Tensor torch의 Tensor는 3가지 방식으로 만들 수 있습니다. 직접 data를 입력 data = torch.tensor([[1,2] , [3,4]]) print(data) data = torch.tensor(np.array([[1,2] , [3,4]]))...","categories": [],
        "tags": [],
        "url": "/PyTorch-tutorial-tensor-tutorial/",
        "teaser": null
      },{
        "title": "Move Blog to Jekyll",
        "excerpt":"Hi , this is the my first post in the Jekyll blog. I have written my CS blog in tistory. I have thought tistory is uncomfortable to write CS blog. So , is there any alternative ? Yes, it’s the Jekyll What is Jekyll How to make github.io with Jekyll?...","categories": ["Blog Jekyll"],
        "tags": ["Blog","jekyll","Github","Git"],
        "url": "/blog%20jekyll/Move-Blog-to-Jekyll/",
        "teaser": null
      },{
        "title": "Data Engineering 개요",
        "excerpt":"DataEngineering 이란? Data Engineering은 고급 레스토랑에서 음식을 서빙하는 것 이랑 유사하다고 생각합니다. 매일 아침 4시 새벽에 식자재를 식당 뒷문으로 들여옵니다. 막내들이 식자재를 손질합니다.(ex,조개 해감, 야채 씻기) 밑준비를 합니다. 준비가 된 재료들을 용기에 담아놓습니다 사수 한테 검사를 맡습니다. 그리고, 혼납니다. 오픈 시간이 되면서, 헤드 셰프가 오더를 하기 시작합니다. 준비된 재료를 이용해서...","categories": ["Data Engineering기본"],
        "tags": ["Data Engineering","DataLake","DataWarehouse","GCP"],
        "url": "/data%20engineering%EA%B8%B0%EB%B3%B8/What-is-de/",
        "teaser": null
      },{
        "title": "Programmers[weekly3]",
        "excerpt":"개요 Programmers 위클리 3주차를 풀었는데 , 내 기준에선 상당히 어려웠다. 내가 약점인 부분이 문제로 나왔다고 생각해서 자세하게 보게 되었다. 이건 아카이브 용으로 작성한 자기회고글이다. 남을 이해시킬려 작성한 글이 아니기 때문에 전문성이나 정확성이 떨어질 수 있다. 잘못된 점이 있는거같으면 깃허브나 이메일로 문의를 부탁합니다. 문제 현재 게임 보드의 상태 game_board, 테이블 위에...","categories": ["Programmers"],
        "tags": ["Algorithm","CodingTest","CodingInterview"],
        "url": "/programmers/weekly3/",
        "teaser": null
      },{
        "title": "Quiver:Plot vector",
        "excerpt":"개요 Linear Algebra를 공부하면서 , 벡터랑 그래프를 그려서 첨부해야할 상황이 많이 생겼습니다. latex 문법에 tikz 를 이용해서 삽입을 할 수 있다고는 하는데, 적기가 좀 불편해서 , matplotlib의 quiver를 이용해서 벡터를 그리기로 하였습니다. matplotlib.pyplot.quiver — Matplotlib 3.5.0 documentation﻿ Quiver quiver([X, Y], U, V, [C], **kw) 제가 벡터를 Parameters X , Y...","categories": ["Data Visualization"],
        "tags": ["Visualization","Machine Learning","Linear algebra","vectors"],
        "url": "/data%20visualization/Quiver/",
        "teaser": null
      },{
        "title": "Matrices in linear Algebra:operating on vectors",
        "excerpt":"This post is based on MathMatics for Machine Learning in Coursera. How Matrices transforms spaces we studied idea that solving simultaenous equation problems using matrices. And , we know that colums on matrices are transformed basis vectors. Now, we ‘ll see how matrices transform spaces. we assume that \\(e_1=(1,0) ,e_2=(0,1)...","categories": ["Matrix Transformation"],
        "tags": ["Mathmathics","Machine Learning","Coursera","Linear algebra","matrix","vectors"],
        "url": "/matrix%20transformation/How-Matrices-transforms-spaces/",
        "teaser": null
      },{
        "title": "TransferLearning in NLP",
        "excerpt":"개요 deeplearning.ai 의 NLP 특화 과정의 일부를 듣고 기록하는 것입니다. 본 글은 상업적인 목적으로 작성하는 것이 아니기 때문에 , deeplearning.ai 의 post를 이용하도록 하겠습니다. QA task Qa task는 크게 2가지 방법으로 문제를 해결합니다. 1.Context-based QA context-based는 Question 과 context 가 동시에 주어지면 Model에서 이를 읽고 정답을 찾아내는 것입니다. 2. Closed...","categories": ["DeepLearningainlp qa"],
        "tags": ["Machine Learning","Coursera","deep learning","nlp"],
        "url": "/deeplearningainlp%20qa/transferlearning-in-nlp/",
        "teaser": null
      },{
        "title": "블록중심회전 vs 점 중심회전",
        "excerpt":"블록 그 자체 회전 vs 점 , 블록 중심 회전 뭔가 이것을 표현할 방법이 없어서 나의 언어로 열심히 표현을 해보았습니다. 점 중심 회전 제가 말한 점 중심 회전은 원점 중심의 회전을 의미하는 것입니다. 원점을 중심으로 회전을 하면 위의 그림과 같은 방식으로 회전하게 됩니다. 이 때 에는 회전 변환의 공식을 적용하면...","categories": ["Codingtechnique"],
        "tags": ["Algorithm","CodingTest","CodingInterview","technique"],
        "url": "/codingtechnique/blockrotatevspointrotate/",
        "teaser": null
      },{
        "title": "ML landscape",
        "excerpt":"개요 이 포스트는 Orelly 의 Hands on MachineLearning 책 chp1 을 공부하고 필자가 정리하는 글입니다. 아카이브 용으로 적는거라 표현이나 설명이 부족할 수 있습니다. 제가 먼저 정리하고 다시 책을 참조하고 부족한 부분은 채워넣는 식으로 글을 작성합니다. 부족한 부분은 피드백 부탁드리겠습니다. What is ML? 이 책에서 필자는 첫장에서 이러한 질문을 던집니다. ML이란...","categories": ["Handsonmachinelearning"],
        "tags": ["Machine Learning","Handsonmachinelearning","orelly"],
        "url": "/handsonmachinelearning/chp1-MLlandscape/",
        "teaser": null
      },{
        "title": "ModelOptimization Introduction",
        "excerpt":"이 자료는 필자가 만든 자료가 아닌 부스트코스 ai tech 2기 교육자료임을 명시합니다. 이 자료는 상업적으로 이용할 수 없습니다. 모델 경량화란? 기존의 ai는 외부 서버(ex클라우드) 에 데이터를 전송하여 추론을 한후 다시 단말기기(edge-device)에 전달하여 결과를 보여주는 방식이였습니다. 이는 , 단말기기(ex,스마트폰,센서 etc)의 컴퓨팅 파워 , 램 사용량 , 저장공간 , 저전력사용(수명) 등의 제약사항...","categories": ["ModelOptimization"],
        "tags": ["Machine Learning","Boostcourse","deep learning"],
        "url": "/modeloptimization/modelopmization1/",
        "teaser": null
      },{
        "title": "기본 알고리즘의 중요성",
        "excerpt":"기존 알고리즘을 사용할 때 주의점 BFS,DFS,Dijkstra,etc…정말 기존 알고리즘 중 많이 나오는 알고리즘 top3라고 개인적으로 생각한다. 필자의 경우 DFS는 recursion , while 반복문 2가지 방법으로 모두 구현할 수 있다. 지금부터 ,필자가 얘기할 건 굉장히 무언가 기초적인 사실이고 남들은 다 아는 얘기일 가능성이 높다. 필자는 어렴풋이 인지하지만 적용은 안했던 사실을 회고느낌으로 적는...","categories": ["Codingtechnique"],
        "tags": ["Algorithm","CodingTest","CodingInterview","technique"],
        "url": "/codingtechnique/caution-for-basic-block/",
        "teaser": null
      },{
        "title": "BigQuery Intro",
        "excerpt":"Bigquery Google Cloud Platform 에서 제공해주는 Data Warehouse service중 하나이면서 , Big data analytics tool입니다. Big data analytics를 위해 쓸모있는 structured data , semi-structured data가 Bigquery로 loading 됩니다. query petabyte in seconds Bigquery는 petabyte 단위의 데이터를 초단의 latency로 query할 수 있습니다. Ecosystem ETL and Data processing 다양한 Sass(Software as a...","categories": ["Bigquery"],
        "tags": ["Google Cloud","GCP","Bigquery"],
        "url": "/bigquery/Bigquery_intro/",
        "teaser": null
      },{
        "title": "AutoML 이론",
        "excerpt":"이 자료는 필자가 만든 자료가 아닌 부스트코스 ai tech 2기 교육자료임을 명시합니다. 이 자료는 상업적으로 이용할 수 없습니다. Conventional DL Training Pipeline 전통적인 DeepLearning Training Pipeline은 사람이 HyperParameter Tuning,Model selection을 하고 train/evaluation을 하는 과정의 반복이였습니다. 위 그림은 위 과정을 재밌게 표현한 짧은 컷툰입니다. AutoML 위를 자동화 해보자 하고 나온게 AutoML...","categories": ["ModelOptimization"],
        "tags": ["Machine Learning","Boostcourse","deep learning"],
        "url": "/modeloptimization/optimization3/",
        "teaser": null
      },{
        "title": "BigQuery Access Control",
        "excerpt":"이 내용은 Courser GCP Professional Data Enigneering 강의및 Google Cloud Blog 및 Google CLoud Docs 기반으로 작성한 글입니다. Query and Data in Bigquery Bigquery는 data를 가지고 Query문을 실행하면 result가 나오는 구조라고 개인적으로 생각합니다.이 글에서는 , Data 와 Query에 대한 Access Control을 어떻게 하는지 알아 보겠습니다. Data Access Control Data Organization...","categories": ["Bigquery"],
        "tags": ["Google Cloud","GCP","Bigquery"],
        "url": "/bigquery/Query-and-Data-in-Bigquery/",
        "teaser": null
      },{
        "title": "Metaverse Introductions",
        "excerpt":"메타버스(metaverse)에 대해 긍정적으로 생각하면서 , 이 technology에 대해 굉장히 관심이 많습니다. 제가 여러 훌륭한 youtube 컨텐츠를 보면서 제가 메타버스를 무엇이라 생각하는지를 적어보겠습니다. 이번 글은 시각적인 예가 도움이 많이 될거 같아서 동영상 링크와 사진을 많이 첨부했습니다. 메타버스란? 위키피디아를 찾아보았는데 , 아직 뚜렷한 정의는 확립되지 않았습니다. 현실세계와 같은 사회적.경제적 활동이 통용되는 세계...","categories": ["None"],
        "tags": ["Metaverse","GCP","Meta","Ironman"],
        "url": "/none/what-is-metaverse/",
        "teaser": null
      },{
        "title": "AutoML:Optumna",
        "excerpt":"이 자료는 필자가 만든 자료가 아닌 부스트코스 ai tech 2기 교육자료임을 명시합니다. 이 자료는 상업적으로 이용할 수 없습니다. Optuna란? optuna를 강의에서 사용하게 된 이유는 SOTA 알고리즘이 구현이 되어 있고 , 병렬화가 용이하고 , Conditonal 하이퍼파라미터 구성이 용이하다는 장점이 있습니다. 또한 , 이를 프레임워크가 아닌 라이브러리 형식으로 import 해서 사용이 용이하다는...","categories": ["ModelOptimization"],
        "tags": ["Machine Learning","Boostcourse","deep learning"],
        "url": "/modeloptimization/modelopt4Optuna/",
        "teaser": null
      },{
        "title": "Pytorch SubSampling : Dataloaer & SubSampler, ranodm_split , Subset",
        "excerpt":"Pytorch Subsampling 파이토치로 model optimization 을 하다보면 많은 양의 실험을 진행해야 합니다. 실제로, 학습을 해서 사용하기 보다는 POC(proof of correctness) 가 목적이라고 볼 수 있습니다. 진짜로 학습을 하기 보다는 최적의 Architecture , Augmentation 조합, hyperparameter를 찾는 것이기 때문에 탐색이 주 목적입니다. 따라서 , tiny set에 대하여 experiment를 진행하는 것이 더...","categories": ["Pytorch"],
        "tags": ["Machine Learning","Pytorch","deep learning"],
        "url": "/pytorch/Pytorch-Subsampling/",
        "teaser": null
      },{
        "title": "Intro,polynominal curve",
        "excerpt":"pattern recognition and machine learning이라는 책으로 high-level 관점에서의 저의 deep learning 지식을 좀 더 넓게 그리고 low-level에서 바라보려는 시도를 하려고 합니다. 이 블로그 포스트는 책을 공부하고 복습하고 덮고 글로 작성한 것입니다. 놓친 부분은 책을 보고 추가해서 정리합니다 digit classifier 이 책은 처음에 손글씨 digit classifier의 예제로 시작을 합니다. 28 X...","categories": ["PatternRecognitionML"],
        "tags": ["Machine Learning","Pattern Recognition and Machine Learning","deep learning"],
        "url": "/patternrecognitionml/chp1-intro/",
        "teaser": null
      },{
        "title": "When to change Dev/test sets and metrics",
        "excerpt":"Change Metrics Example : Cat vs Dog 저희가 Cat, Dog 를 검출하는 Simple model을 만들어서 배포하는 시나리오를 가정해보겠습니다. Algorithm Classification Error A algorithm 3% B algorithm 5% Dev/test에서 이러한 지표가 나왔습니다. 자연스럽게 A를 선택하게 됩니다. 하지만, 현실의 문제는 그렇게 간단하지 않습니다. 만약에, A 알고리즘은 음란물을 검출할 수 있습니다. 하지만, B...","categories": ["DLS_C3"],
        "tags": ["Machine Learning","Coursera","deep learning"],
        "url": "/dls_c3/When-to-change-testdev-and-metrics/",
        "teaser": null
      },{
        "title": "Huggingface tutorial: Tokenizer summary",
        "excerpt":"Huggingface tutorial 시리즈 : tokenizer Huggingface tutorial 시리즈중 tokenizer 편을 듣고 정리한 글입니다. Summary of the tokenizers What is tokenizer tokenizer란 sentence를 sub-word 혹은 word 단위로 쪼갠후 이를 look-up-table을 통해 input ids로 변환하는 프로그램입니다. Huggingface tutorial에서는 특히 transfomers 기반의 모델들에 사용되는 tokenizer를 살펴보게 됩니다. word-based tokenizer word-level 단위로 tokenizing하는 tokenizer는...","categories": ["Huggingface"],
        "tags": ["Machine Learning","Huggingface","deep learning"],
        "url": "/huggingface/Huggingface-tutorial-tokenizer/",
        "teaser": null
      },{
        "title": "Nas from scratch using Optuna",
        "excerpt":"NAS란? NAS란 neural architecture search의 약자입니다. NAS는 이미 존재하는 블록 모듈들을 가지고 이들을 적절히 조합해서 새로운 architecture를 찾는 시도입니다. 이를 컴퓨터가 만들다 보니 인간의 직관과 거리가 먼 모델이 나올 수 있습니다. 하지만, 컴퓨터가 더 잘 찾는 여러 사례가 나오고 있기 때문에 가치 있는 연구주제라고 생각이 듭니다. Boost Course에서는 실습 파일로...","categories": ["ModelOptimization"],
        "tags": ["Machine Learning","Boostcourse","deep learning"],
        "url": "/modeloptimization/nas_from_scratch/",
        "teaser": null
      },{
        "title": "정해진 데이터 vs 임의의 데이터 집합",
        "excerpt":"정해진 데이터 vs 임의의 데이터 집합 자료구조(ex,tree)문제를 풀다 보면 데이터 집합이 정해진 문제가 나오거나 조건을 만족하는 모든 데이터의 집합을 구하는 문제가 나옵니다. 예전에, 저는 이러한 문제들을 자료구조라는 카테고리에 묶어두었는데, 자료구조에 대한 내용은 데이터 집합이 정해졌 때 행해지는 operation에 대한 정의라고 생각합니다. 정해진 데이터 자료구조에 대한 내용을 보면 자료구조의 construction, insert,delete...","categories": ["Codingtechnique"],
        "tags": ["Algorithm","CodingTest","CodingInterview","technique"],
        "url": "/codingtechnique/ds-vs-sds/",
        "teaser": null
      },{
        "title": "Momentum Rmsprop Adam",
        "excerpt":"Momentum   RMSprop   Adam   Adamw   ","categories": [],
        "tags": [],
        "url": "/Momentum-rmsprop-adam/",
        "teaser": null
      },{
        "title": "LeetCode 96 unique BST",
        "excerpt":"96. Unique Binary Search Trees leetcode 96번 문제를 리뷰 해보도록 하겠습니다. 중요한 키포인트만 잡고 세세한 부분은 넘어가도록 하겠습니다. Description Given an integer n, return the number of structurally unique BST’s (binary search trees) which has exactly n nodes of unique values from 1 to n. key point 이 문제는 DP로...","categories": ["LeetCode"],
        "tags": ["Algorithm","CodingTest","CodingInterview"],
        "url": "/leetcode/96bst/",
        "teaser": null
      },{
        "title": "Flask:Sentiment Classifier tutorial",
        "excerpt":"Flask:Sentiment Classifier tutorial ODQA system을 만들어서 실제로 배포를 하려고 합니다. 하지만, 한 번도 배포를 해본 적이 없기에 우선 web으로 배포를 시도해볼려 합니다. Flask로 deep learning 모델을 배포하는 과정에 대한 배경지식이 없기에 top-down 식으로 빠르게 high-level view를 생성하기 위해서 간단한 sentiment classifier tutorial을 진행했습니다. 이를 이용해서 odqa 시스템 배포로 확장해 나갈...","categories": ["Huggingface"],
        "tags": ["Machine Learning","Huggingface","deep learning","Flask"],
        "url": "/huggingface/FlaskSentiment-Classifier-tutorial/",
        "teaser": null
      },{
        "title": "Batch vs Mini batch vs Stochastic",
        "excerpt":"이 내용은 DeepLearning Specialization , BoostCourse AI tech 2기 내용을 듣고 제 맘대로 정리한 내용입니다. Batch Size란? batch size란 GD(gradient Descent) 로 optimization을 진행할 때 얼마만큼의 data에 대해 loss를 계산해서 update할지 정하는 hyperparameter 입니다. 이 batch size를 얼마나 주느냐에 따라 각각의 장.단점 및 특징이 있습니다. 이에 대해 알아보고자 합ㄴ다. Batch...","categories": ["Optimization"],
        "tags": ["Machine Learning","batch size","deep learning"],
        "url": "/optimization/Batch-Size/",
        "teaser": null
      },{
        "title": "Batch Norm introduction",
        "excerpt":"This article was expressed in my own words after listening to the DLS batch norm lecture. Input Normalization 과 유사점 이전에 보았던 Input Normalization은 Input 의 range(scale)를 조정해줘서 , weight의 update가 균일하게 끔 되게 해주었습니다. 따라서, 상대적으로 수렴이 빠르게 되도록 해주는 역할을 하였습니다. 왜 batch normalization을 사용하는가? batch normalization은 이를...","categories": ["Optimization"],
        "tags": ["Machine Learning","batch size","deep learning"],
        "url": "/optimization/Batch-Normalization-intro/",
        "teaser": null
      },{
        "title": "Why  Batch Norm Works",
        "excerpt":"이 글은 Coursera의 batch norm강의를 듣고 작성한 강의 입니다. 강의 내용을 그대로 정리하기 보다는 나의 말로 표현한 글입니다. Why batch norm works 저번 글에서는 Batch Norm을 어떻게 implementation을 하고 어떤 learnable parameters를 설정하는지에 대한 내용을 적었습니다. 이번 글은 batch norm이 왜 작동하는 지를 좀 더 low-level에서 바라볼 예정입니다. Covariance shift...","categories": ["Optimization"],
        "tags": ["Machine Learning","batch size","deep learning"],
        "url": "/optimization/Why-batchnorm-works/",
        "teaser": null
      },{
        "title": "Data Loading into Bigquery",
        "excerpt":"이 내용은 Course Preparing for Google Cloud Certification: Cloud Data Engineer 특화 과정 강의 Part2의 Data Warehouse를 듣고 제 표현대로 강의 내용을 정리한 것입니다. Closed Book으로 적고 부족한 내용은 다시 검토하면서 채워넣습니다. Data Loading Bigquery에 Query를 요청하려면 Data에 access 할 수 있어야 합니다. 여러 방법이 있는데 , 그 중 Bigquery에...","categories": ["Bigquery"],
        "tags": ["Google Cloud","GCP","Bigquery"],
        "url": "/bigquery/Data-Loading-into-Bigquery/",
        "teaser": null
      },{
        "title": "Batch Norm in Neural Network Training and Test",
        "excerpt":"Batch Norm in Neural Network Training and Test 이번 글에서는 실제로 Neural Network Training 과 Test에서 Batch Norm을 어떻게 적용하는지 알아보도록 하겠습니다. Adding Batch Norm into Nueral Network \\[X^{[1]} \\implies W^{[1]} , b^{[1]} \\implies Z^{[1]} , a^{[1]} = g(Z^{[1]}) \\implies W^{[2]} ,b^{[2]} \\implies Z^{[2]} , a^{[2]} = g(Z{[2]}) \\implies ...\\]...","categories": ["Optimization"],
        "tags": ["Machine Learning","batch size","deep learning"],
        "url": "/optimization/Batch-Norm-in-Neural-Network/",
        "teaser": null
      },{
        "title": "Huggingface:TrainerCallback",
        "excerpt":"TrainerCallback Huggingface에서 Trainer라는 Class는 모델의 학습을 굉장히 쉽게 해줍니다. 하지만, logging 방식이 정해져 있기 때문에, 상황에 따라 logging을 다르게 할 수 있는 방법을 찾다가 TrainerCallback이라는 것을 알게 되었습니다. Huggingface에서 TrainerCallback이라는 Class를 제공해줍니다. TrainerCallback class를 subclassing해서 다양한 Callback Class들이 제공되고 있습니다. subclassing을 통해 다양한 callback method를 override 할 수 있기 때문에...","categories": ["Huggingface"],
        "tags": ["Machine Learning","Huggingface","deep learning","Callback"],
        "url": "/huggingface/Trainer-Callback/",
        "teaser": null
      },{
        "title": "Huggingface:Datasets",
        "excerpt":"Huggingface Datasets Huggingface에서는 Datasets라는 Module을 제공해줍니다. 이번 글에서는 Huggingface 의 Datasets에 대한 소개와 제가 자주 쓰는 간단한 method 및 attribute들을 소개해볼려 합니다. Datasets Arrow Huggingface Datasets는 dataset을 external filesystem으로 부터 loading을 하게 되면 local 에 arrow로 caching합니다. Arrow는 대용량의 data를 빠르게 processing 하기 위해 고안되었습니다. Arrow는 특히 column orientated data에...","categories": ["Huggingface"],
        "tags": ["Machine Learning","Huggingface","deep learning","Dataset"],
        "url": "/huggingface/Huggingface-Datasets/",
        "teaser": null
      },{
        "title": "Pretrained Language Model",
        "excerpt":"PLM(Pretrained Language Model)   ","categories": ["NLP"],
        "tags": ["Machine Learning","NLP","deep learning"],
        "url": "/nlp/dd/",
        "teaser": null
      },{
        "title": "Module and Package",
        "excerpt":"Python Module and Pacakage BoostCampAi Tech에서 최종프로젝트를 진행하였습니다. 제 개인적인 생각으로는 중형규모의 프로젝트였습니다. user flow , system design, prototype , serving의 단계를 거친 프로젝트 였기에 , 여러 module, 이를 넘어선 package 단위로 프로젝트를 관리하게 되었습니다. 초기에는 , 모듈을 import 할 때 absolute path로 관리를 했었습니다. module 및 package를 관리하기 위해서...","categories": ["Python"],
        "tags": ["Python","Module","Package"],
        "url": "/python/Python-Module-and-Pacakage/",
        "teaser": null
      },{
        "title": "Pretrained Language Model",
        "excerpt":"PLM(Pretrained Language Model) Billion 단위의 Parameter로 이루어진 PLM(Pretrained Language model)이 오픈소스로서 공유되어 있습니다. Bert 등장이후, 언어의 representation을 학습하고 이를 다양한 downstream task에 맞게 fine-tuning의 시도가 계속 이어지고 있습니다. 그리고, 대형 연구기관(Ex. Google, DeepMind, OpenAI…. etc) 에서는 계속해서 대형 PLM 을 논문으로 발표하고 있습니다. 저도 예전에는 대형 PLM을 만들 줄 알아야...","categories": ["NLP"],
        "tags": ["Machine Learning","NLP","deep learning"],
        "url": "/nlp/PLM(Pretrained-Language-Model)/",
        "teaser": null
      },{
        "title": "Python Scope",
        "excerpt":"Scope 파이썬에서 함수를 만들고 , 모듈을 만들고 ,패키지를 만듭니다. 어떤 Module(혹은 단순 script)를 실행하면, 여러 변수, 즉 name 들을 namespace에 할당하게 됩니다. 저는, 여태까지 이러한 과정들을 함수안에 생성하면 지역변수 , 함수밖에 전역변수 이 두 가지 카테고리로 분류했습니다 . 파이썬을 좀 더 디테일 하게 사용하기 위해서 이러한 과정들을 좀 더 디테일하게...","categories": ["Python"],
        "tags": ["Python","Module","Package.Scope"],
        "url": "/python/Scope-basic/",
        "teaser": null
      },{
        "title": "CI(Continuous Integration)",
        "excerpt":"CI에 대해 한번 다뤄보았습니다. github action에 대해서 간단하게 적을 수 있을거라 생각했는데, 생각보다 내용을 더 자세하게 다루는게 좋을거 같다고 생각되어서 다른 글에서 githb action을 자세하게 다루고, 이번 글에서는 간단히 소개만 하도록 하겠습니다. Why CI 개발자들은 여러 개발자가 팀을 이루어서 프로젝트를 진행하게 됩니다. Alice, Bob이 서로 간에 소통없이 자기가 담당하는 기능을...","categories": ["CI/CD"],
        "tags": ["SoftwareEngineering","CI/CD"],
        "url": "/ci/cd/Why-CI/",
        "teaser": null
      },{
        "title": "Weighted Average and Momentum",
        "excerpt":"이번 글은 DLS에서 다룬 Exponentially Weighted Average , Momentum을 다루어 볼려 합니다.   Weighted Average   \\[V_n =  \\beta \\theta_n + (\\beta -1) V_{n-1}\\]  Weighted Average in statistical view   Momentum   omit B   ","categories": ["Optimization"],
        "tags": ["Machine Learning","batch size","deep learning"],
        "url": "/optimization/Weighted-Average/",
        "teaser": null
      }]
