<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-01-14T06:21:41+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Woongjoon_AI</title><subtitle>AMazing developer</subtitle><author><name>Choi Woongjoon</name></author><entry><title type="html">Train,Dev,Test의 error를 어떻게 해석해야 할까요?</title><link href="http://localhost:4000/dls_c3/What-to-do-we-see-error/" rel="alternate" type="text/html" title="Train,Dev,Test의 error를 어떻게 해석해야 할까요?" /><published>2024-01-11T00:00:00+09:00</published><updated>2024-01-11T00:00:00+09:00</updated><id>http://localhost:4000/dls_c3/What-to-do-we-see-error</id><content type="html" xml:base="http://localhost:4000/dls_c3/What-to-do-we-see-error/">&lt;p&gt;프로젝트에서 Machine Learning Model을 사용하기로 했습니다. 그러면, Model을 개발할 때, 어떤 Model을 고르고, 데이터를 증강하는지, 어떤 알고리즘을 쓰는지 등등이 중요해집니다. 이는 모델이 Over fit인지, Variance가 높은지 판단해야 좋은 선택을 할 수 있습니다.Andrew 교수님의 DeepLearning Specialization에서 이에 대한 내용을 다루었고 이를 정리했습니다.&lt;/p&gt;

&lt;h1 id=&quot;error-rate를-확인하면-어떻게-해야할까&quot;&gt;Error-rate를 확인하면 어떻게 해야할까?&lt;/h1&gt;
&lt;p&gt;Error-rate를 train,dev, test에 대해서 logging 할 것입니다. 이 log 값들을 가지고 어떻게 해야 할지 결정하기 위해서는 몇 가지 용어들을 정의해야 합니다.&lt;/p&gt;

&lt;h2 id=&quot;term&quot;&gt;Term&lt;/h2&gt;
&lt;p&gt;여기서는 human-level performance와 avoidable bias라는 용어의 정의와 이에 대해 간략한 설명을 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;human-level-performance&quot;&gt;Human-level performance&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://onedrive.live.com/embed?resid=7E81BBCD99889380%217819&amp;amp;authkey=%21AFm_FOTf9RHr9A4&amp;amp;width=611&amp;amp;height=351&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Machine Learning의 성능은 시간이 지나면서 사람의 능력을 뛰어넘게 되었습니다. 하지만, 이론상 최고 성능에는 도달하지 못하고 계속 근접하게 됩니다. 이론상 최고치를 Bayes optimal error라고 합니다. 여기서, 사람의 능력을 human level performance라고 합니다. Bayes error는 이론상의 최대치이고 계산할 수 있지만 조건에 따라 계산이 매우 복잡하여 수치적으로 근사하는 것이 최선일 수 있습니다. 즉, &lt;strong&gt;&lt;em&gt;언제나 정확한 Bayes error 값을 알 수 없습니다.&lt;/em&gt;&lt;/strong&gt; Bayes error를 human level performance가 넘을 수는 없지만, human level performance는 Bayes error에 상당히 근접해 있습니다. 따라서, human level performance를 Bayes error의 approximation이라 여기고 Bayes error 대신 사용하게 됩니다.&lt;/p&gt;

&lt;p&gt;머신러닝 에서는 human level performance에 도달하기 전까지는 모델의 performance를 빠르게 올릴 수 있습니다. 왜냐하면, bias, variance를 설정할 수 있으므로 목표를 어떻게 잡아야 할지 명확히 알 수 있습니다. 하지만, human level performance를 넘게 된다면 performance를 끌어올리기 어려워집니다. Bayes error를 정확히 알 수 없기에 목표 지점을 정확히 잡을 수 없습니다. 특히, Online Ads, Recommendation, Loan Approval 같은 &lt;strong&gt;Structured Data&lt;/strong&gt; 을 활용하는 Model은 이미 사람을 뛰어넘었다고 알려져 있습니다. 이러면, bias, variance를 활용한 방법이 아닌 다른 방법들을 마련해야 합니다. 하지만, 시각, 청각, 사고 같은 &lt;strong&gt;자연적 인지&lt;/strong&gt; 에서는 아직 인간을 딥러닝이 따라잡지 못하고 있습니다. 따라서, human-level performance를 아는 것이 중요하다고 할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;avoidable-bias&quot;&gt;Avoidable bias&lt;/h3&gt;
&lt;p&gt;avoidable bias는 &lt;strong&gt;Bayes error approximation 과 training error 간의 차이&lt;/strong&gt;입니다. 여기서, approximation은 human level performance가 됩니다. 이 avoidable bias를 bias로 설정하고,  variance는 human level performance와 dev error와의 차이로 설정해서
모델의 performance를 높이기 위한 전략을 세우게 됩니다. 이때, avoidable bias는 0 이상이어야 하므로, training error의 upper bound가 human level performance임을 내포하고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;bias-reduction-or-variance-reduction-&quot;&gt;bias reduction or variance reduction ?&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;error type&lt;/th&gt;
      &lt;th&gt;Approach&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;High Avoidable Bias&lt;/td&gt;
      &lt;td&gt;- Traing Bigger Model  &lt;br /&gt; - Train Longer  &lt;br /&gt; - Better Optimization Algorithm  &lt;br /&gt; - Architecture,hyperparameter search&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;High Variance&lt;/td&gt;
      &lt;td&gt;- More Data &lt;br /&gt; - regularization &lt;br /&gt; - Architecture,hyperparameter search&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Avoidable bias가 높다는 것은 모델이 Training data를 잘 학습하지 못한다는 뜻이므로, parameter가 더 많은 모델을 사용하거나 훈련을 더 길게 하거나 optimization 알고리즘이 다른 걸 사용할 수 있습니다.
Variance가 높다는 것은 모델이 일반화를 못했다는 뜻이므로, 더 많은 데이터를 수집하거나 Regularization 기법을 사용할 수 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;error를-분석하자&quot;&gt;Error를 분석하자&lt;/h1&gt;
&lt;p&gt;bias문제인지 variance 문제인지  error-rate 를 통해 판단이 되었습니다. 그렇다면, 구체적인 error의 내용을 분석해서 구체적인 전략을 세워야 합니다.&lt;/p&gt;

&lt;h2 id=&quot;error-label-check&quot;&gt;error-label check&lt;/h2&gt;
&lt;p&gt;dev, test, train의 error를 사람이 확인해서 구체적인 insight를 얻을 수 있습니다. 예를 들어, 동물을 분류하는 multi classifier 문제의 경우에 아래와 같은 식으로 어떻게 잘못 분류했는지를 검사할 수 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Image&lt;/th&gt;
      &lt;th&gt;Dog&lt;/th&gt;
      &lt;th&gt;Cat&lt;/th&gt;
      &lt;th&gt;….&lt;/th&gt;
      &lt;th&gt;Comments&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;\(\checkmark\)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Pitball&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;\(\checkmark\)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Lion&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;체크를 하다 보면, labeling이 잘못되어 있는 경우가 있을 수 있습니다. 이 경우에는 label miss라는 마크를 따로 달아두는 것이 좋다고 합니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Image&lt;/th&gt;
      &lt;th&gt;Dog&lt;/th&gt;
      &lt;th&gt;Cat&lt;/th&gt;
      &lt;th&gt;….&lt;/th&gt;
      &lt;th&gt;miss label&lt;/th&gt;
      &lt;th&gt;Comments&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;\(\checkmark\)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Pitball&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;\(\checkmark\)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Lion&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;\(\checkmark\)&lt;/td&gt;
      &lt;td&gt;miss labeling&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;이러한 analysis를 통해 insight를 얻게 되면, 바로 성능향상 작업에 들어가서는 안 됩니다. 특정 error나 mislabeling이 나왔다고 해서 전부 고치는 것이 항상 정답은 아닙니다. 예를 들어, dog를 잘못 분류하는 예제가 10개가 나왔고, dog를 잘 분류하도록 알고리즘을 설계하는 작업에 착수했습니다. 하지만,  이 예제가 전체 오답률의 1퍼센트라면 이는 미미한 성능향상을 끌어낼 것입니다. 또, labeling을 잘못되어 있어서 이를 수정하는 데 자원을 소모했습니다. 그렇지만, 이 비율이 2퍼센트 정도라면 큰 성능향상을 끌어내지 않을 것입니다.
만약에, 특정 예제나 miss labeling이 비율이 오답의 30%~40%라면 유의미한 성능향상을 보일 것입니다.
error label을 검사할시 몇 가지 tip이 있습니다.&lt;/p&gt;

&lt;p&gt;error label을 사람이 체크하고 수정할 시, dev set에 했던 process를 test set에서도 해야 합니다. machine learning model은 dev, test가 같은 distribution에서의 data라 예상하고 model의 성능을 평가합니다. 따라서, dev에서도 일반화가 된다면 test에서도 일반화가 되어야 합니다. dev와 test가 같은 distribution을 가져야 하므로, dev에서 label을 고쳤다면, test에서도 label을 고쳐줍니다.&lt;/p&gt;

&lt;p&gt;error를 확인할 때 맞춘 것도 보는 것이 도움이 됩니다. Model이 운이 좋아서 정답을 맞힌 가능성이 있기 때문입니다. 만약에, 틀린 예제만을 점검할 시, 모델의 error 추정치가 과대평가 될 수 있습니다. 하지만, 이는  Model의 성능이 좋으면 좋을수록 하기 어렵습니다. 예를 들어, 모델의 정확도가 98퍼센트이고 error 비율이 2퍼센트라면면 98퍼센트의 데이터를 보는 것은 많은 자원이 소모되기 때문입니다.&lt;/p&gt;

&lt;p&gt;실제로는, training data의 error를 확인하는 것이 정석이지만, dev ,test에 비해 data의 size가 상당히 크기 때문에 자원이 상당히 많이 소모됩니다. 딥러닝 알고리즘은 random 한 error에 robust 하므로 training data의 random 한 error에 대해서는 영향을 거의 받지 않을 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;build-system-first&quot;&gt;Build System First&lt;/h2&gt;
&lt;p&gt;실제로 ML project를 임할 때에는 error analysis에 집중하기 보다는 bias, variance를 통해 model의 performance를 향상하게 시키는 system을 구축하는 것이 중요하다고 합니다. error analysis나 distribution 차이는 그다음에 해결해도 괜찮다고 합니다. 우선은, model performance를 향상하게 시키는 system을 구현하는 것이 훨씬 중요하다고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;mismatched-data-distribution&quot;&gt;Mismatched data distribution&lt;/h2&gt;
&lt;p&gt;위의 Error Analysis 과정을 거치면서, 특정 label이나 mislabel 문제가 아님이 밝혀질 수 있습니다. Train 과 dev, test의 distribution 차이에서 오는 variance의 문제일 수도 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;distribution-eqaulity&quot;&gt;Distribution Eqaulity&lt;/h3&gt;
&lt;p&gt;예를 들어, 모바일 앱에서 구동하는 ML application을 개발한다고 하겠습니다. Train Data 약 20만 개를 web에서 수집하고, dev, test에는 mobile app에서 수집한 data 1만 개를 각각 5천개씩 나누어서 사용했습니다. 이렇게 되면, train과 dev, test 간의 distribution의 차이로 문제가 발생할 수 있습니다. web에서의 image data는 화질이 좋지만,, mobile app의 image data는 화질이 좋지 않을 가능성이 있습니다.&lt;/p&gt;

&lt;p&gt;이때 , train, dev,test data를 단순하게 shuffle 해서 같은 distribution으로 만들어 주는 방법을 사용할 수 있습니다. 하지만, 이 새로운 data는 mobile data 반영률이 약 4.7퍼센트 정도밖에 되질 않을 것입니다. ML model이 학습하고자 하는 distribution은 mobile의 distribution인데, model은 전혀 다른 distribution을 target으로 삼고 있는 것입니다.
대신에, dev, test를 각각 2500개 정도로 설정하고 5천 개를 train에 포함하므로 train의 distribution을 dev, test에 더 가깝게 만드는 방법을 사용할 수 있습니다.  하지만, 여전히 train과 dev, test의 distribution은 다릅니다.&lt;/p&gt;

&lt;p&gt;어떤 방법을 적용할지는 팀의 상황과 project의 제한조건 등에 따라서 다를 것이므로 , 정답이 없다고 여겨집니다. 하지만, Andrew 교수님은 후자의 방법이 장기적으로 더 좋은 효과를 보일 것이라 얘기하십니다.&lt;/p&gt;

&lt;h3 id=&quot;data-augmentation-and-collect-data&quot;&gt;Data Augmentation and Collect Data&lt;/h3&gt;
&lt;p&gt;더 많은 data를 확보하는 것도 좋은 방법이 될 수 있습니다. 이때, data augmentation과 collect data를 고려하게 됩니다.
collect data는 말 그대로 data를 추가로 수집을 하는 것입니다. train에 mobile에서의 data가 부족하다면, mobile data를 추가로 수집해서 train data에 포함할 수 있습니다.
Data Augmentation은 기존에 가지고 있는 data로 새로운 data를 합성해 내는 것입니다. 예를 들어, 사람의 목소리와 차의 소음 소리를 결합해 차 속에서 말하는 사람의 목소리를 만들 수 있습니다. 아니면, 기존의 자동차 사진으로 새로운 자동차 사진을 생성해 낼 수 있습니다. 하지만, data augmentation 시 주의할 점이 있습니다. &lt;strong&gt;적은 data로 augmentation 시 train은 여전히 sample space의 작은 subset일 가능성이 있다&lt;/strong&gt;는 것입니다. 차의 소음 소리 data가 만 시간 정도가 있고, 사람의 목소리 data가 100만 시간이 있다고 가정하겠습니다. 여기서, 사용할 수 있는 방법은 차의 소음 소리를 100번 반복해서 사람의 목소리와 합성하는 것입니다. 사람의 귀에는 다 다른 소음 data로 들리지만, computer 입장에서는 전부 같은 소음 data로 보일 수 있습니다. 또한, 20개의 차 이미지로 새로운 차 이미지를 만들어 냈다고 했을 때, 사람의 눈에는 달라 보이지만 computer 입장에서는 같은 차 이미지일 수 있습니다.  small subset만을 학습하게 된다면, model을 sample space의 일부에 overfit이 될 것입니다.&lt;/p&gt;
&lt;h3 id=&quot;train-dev&quot;&gt;Train-dev&lt;/h3&gt;
&lt;p&gt;Train data와 dev, test의 data가 서로 다른 distribution인지 아닌지 모를 수도 있습니다. 이럴 때는, train-dev라는 새로운 data set을 만듦으로써 확인할 수 있습니다. train data에서 무작위로 train-dev를 sampling 하므로 train-dev와 train은 같은 distribution을 갖게 됩니다. 따라서, 이 model이 unseen data에 대해서 예측을 잘하는지를 판단할 수 있게 됩니다.&lt;/p&gt;

&lt;h1 id=&quot;decision-making-in-general-foramt&quot;&gt;Decision Making in General Foramt&lt;/h1&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Error Type&lt;/th&gt;
      &lt;th&gt;Comapre&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;high avoidable bias&lt;/td&gt;
      &lt;td&gt;human-level vs train error&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;high variance&lt;/td&gt;
      &lt;td&gt;train error vs train-dev error&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;data mismatch&lt;/td&gt;
      &lt;td&gt;train-dev error vs dev/test error&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;위에서 작성된 decision 전략에서 train-dev error와의 차이가 추가되었습니다.  알고리즘이 일반화가 잘 되는지를 dev, test가 아닌 train-dev와 비교하는 것이 좀 더 일반화된 형태의 decision making이라 볼 수 있겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;ML project team에서 ML의 새로운 model의 performance를 올릴 때는 아래와 같은 순서를 따른다고 볼 수 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;bias, variance 값에 따라 approach를 적용할 수 있는 system을 만든다.&lt;/li&gt;
  &lt;li&gt;system build 후 error analysis를 통해 distribution mismatch인지 아닌지를 점검한다.&lt;/li&gt;
  &lt;li&gt;distribution이 mismatch가 아니라면 error case 중 비율이 높은 것을 우선시한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning-projects/home/week/1&quot;&gt;DeepLearning Specialization&lt;/a&gt;&lt;/p&gt;</content><author><name>Choi Woongjoon</name></author><category term="DLS_C3" /><category term="Machine Learning" /><category term="Coursera" /><category term="deep learning" /><summary type="html">ML Project structing</summary></entry><entry><title type="html">임의의 숫자의 parameter를 가진 FeedForward Network의 일반항을 정의하고 vectorize하는 과정을 수식으로 도출해보자</title><link href="http://localhost:4000/dlarchitecture/Feed-Forward-Network/" rel="alternate" type="text/html" title="임의의 숫자의 parameter를 가진 FeedForward Network의 일반항을 정의하고 vectorize하는 과정을 수식으로 도출해보자" /><published>2023-12-30T00:00:00+09:00</published><updated>2023-12-30T00:00:00+09:00</updated><id>http://localhost:4000/dlarchitecture/Feed-Forward-Network</id><content type="html" xml:base="http://localhost:4000/dlarchitecture/Feed-Forward-Network/">&lt;h2 id=&quot;why-we-need-understand-neural-network-in-mathematical-view&quot;&gt;Why we need understand Neural Network in Mathematical view?&lt;/h2&gt;
&lt;p&gt;FeedForward Network는 가장 기본적인 형태의 neural net입니다. 이는 노드에 weight matrix를 곱하고 bias를 더 해줌으로써 구현됩니다. neural net을 수식으로 이해함으로써 neural net에 대한 이해도가 올라가기 때문에 수학적으로 이해하는 것이 딥러닝 엔지니어의 역량을 기르는데 많은 도움이 된다고 여겨집니다. 이 글은 임의의 layer의 node의 일반항을 도출하여 vectorize하는 과정을 수식으로 도출할 것입니다 .&lt;/p&gt;
&lt;h2 id=&quot;term-definition&quot;&gt;Term Definition&lt;/h2&gt;
&lt;p&gt;여기서의 Term에 관한 Symbol이나 Notation 방식은 Andrew 교수님의 DeepLearning 강의를 참고하였습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;기호&lt;/th&gt;
      &lt;th&gt;설명&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;\(l\)&lt;/td&gt;
      &lt;td&gt;\(l\)번째  layer  . \(l\) =   0, … , L  은 weights와 bias를 가지는  layer의 번호를 의미한다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(n^{[l]}\)&lt;/td&gt;
      &lt;td&gt;\(l\) 번째 layer의 node의 개수를 의미합니다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(j\)&lt;/td&gt;
      &lt;td&gt;\(j  = 0,..., n^{[l]}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(k\)&lt;/td&gt;
      &lt;td&gt;\(k = 0, .... n^{[l-1]}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(m\)&lt;/td&gt;
      &lt;td&gt;\(m\) 은 training step에서의 batch size 입니다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(i\)&lt;/td&gt;
      &lt;td&gt;\(i = 0,..... m\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(w_{j,k}^{[l]}\)&lt;/td&gt;
      &lt;td&gt;\(l\) 번째 layer의 weight \(W^{[l]}\) , \(W^{[l]} \in \mathbb{R}^{n^{[l]} \times n^{[l-1]} }\) . \(W^{[l]}\) 의 \((j,k)\) 원소를 의미한다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(z_{j,i}^{[l]}\)&lt;/td&gt;
      &lt;td&gt;\(l\) 번째 layer의 bias를 더한 output의 \((j,i)\) 성분입니다 .&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(a_{j,i}^{[l]}\)&lt;/td&gt;
      &lt;td&gt;\(l\) 번째 layer의 activation의 \((i,j)\)성분입니다 .&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(b^{[l]}\)&lt;/td&gt;
      &lt;td&gt;\(l\) 번쨰 layer의 bias입니다.\(b^{[l]} \in \mathbb{R}^{n^{[l]}  }\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(g_{j}^{[l]}\)&lt;/td&gt;
      &lt;td&gt;\(l\) 번째 layer activation function \(g_{j}^{[l]} : \mathbb{R}^{n^{[l]}} \rightarrow \mathbb{R}^{n^{[l]}}\)  , \(g_{j}^{[l]} \in \mathbb{R}^{n^{[l]}  }\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;explanation-in-mathematical-view&quot;&gt;Explanation in Mathematical View&lt;/h2&gt;

&lt;h3 id=&quot;zl-의-vectorization&quot;&gt;\({Z}^{[l]}\) 의 vectorization&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://onedrive.live.com/embed?resid=7E81BBCD99889380%217803&amp;amp;authkey=%21AG3PCmGXC76EdXA&amp;amp;width=1121&amp;amp;height=1233&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다른 교육자료들을 보면 쉽게 설명하기 위해서 3개의 node로 한정짓거나 하는 방식으로 설명을 하게 됩니다. 하지만, 여기서는 일반식을 정의하기 위해서 \(n^{[l-1]}\)개의 node에서  \(n^{[l]}\)개의 node로 변환시키는 \(l\)번째 layer에  대해서 설명을 하겠습니다. (bias는 그림에서만 생략하였습니다 . ) . 위의 그림은 \(l\)번째 layer 를 나타낸 그림입니다. 동그라미가 node이고 연결선이 weight입니다. notation은 &lt;a href=&quot;#term-definition&quot;&gt;Definition&lt;/a&gt;을 참조하시면됩니다.&lt;/p&gt;

&lt;p&gt;여기서 , \(z_{k,i}^{[l]}\) 에는 \(n^{[l-1]}\) 개의 node가 연결되어 있습니다.  &lt;br /&gt;
따라서 ,&lt;/p&gt;

\[\begin{equation}
z_{j,i}^{[l]} = \sum_{k=0}^{n^{[l-1]}}  w_{j,k}^{[l]} \cdot a_{k,i}^{[l-1]} + b_{j}^{[l]} 
\end{equation}\]

&lt;p&gt;위 수식을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(1)&lt;/code&gt; 이라 하겠습니다.&lt;/p&gt;

&lt;p&gt;vector space는 다음과 같이 정의됩니다.  \(\vec{a}_{:, i}^{[l-1]} \in \mathbb{R}^ {n \times {n^{[l-1]}} }, \vec{w}_{j, :}^{[l]} \in \mathbb{R}^ {n \times {n^{[l-1]}}}\)  .&lt;/p&gt;

&lt;p&gt;딥러닝에서는 이러한 multiplication을 sequential하게 하는것이 아닌 parallell 하게 진행합니다.(ex.Numpy) 따라서, 우리는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(1)&lt;/code&gt; 을 vectorization 해야 합니다 .  위 식은 sum을 나타내지만 , 이는 vector \(w_{j,:}^{[l]}\) 와 vector \(a_{:,i}^{[l-1]}\)  의 multiplication이라 볼 수 있습니다. 위 식에서 변수 j의 범위는 \(0&amp;lt;= j &amp;lt;=n^{[l]}\) 입니다.  따라서 , 이를 확장하면 아래와 같이 vectorize할 수 있습니다.&lt;/p&gt;

\[\begin{align*}
\begin{bmatrix}
z_{1, i}^{[l]} \\
\vdots \\
z_{j, i}^{[l]} \\
\vdots \\
z_{n^{[l]}, i}^{[l]}
\end{bmatrix} &amp;amp;=
\begin{bmatrix}
w_{1, 1}^{[l]} &amp;amp; \dots &amp;amp; w_{1, k}^{[l]} &amp;amp; \dots &amp;amp; w_{1, n^{[l - 1]}}^{[l]} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
w_{j, 1}^{[l]} &amp;amp; \dots &amp;amp; w_{j, k}^{[l]} &amp;amp; \dots &amp;amp; w_{j, n^{[l - 1]}}^{[l]} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
w_{n^{[l]}, 1}^{[l]} &amp;amp; \dots &amp;amp; w_{n^{[l]}, k}^{[l]} &amp;amp; \dots &amp;amp; w_{n^{[l]}, n^{[l - 1]}}^{[l]}
\end{bmatrix}
\begin{bmatrix}
a_{1, i}^{[l - 1]} \\
\vdots \\
a_{k, i}^{[l - 1]} \\
\vdots \\
a_{n^{[l - 1]}, i}^{[l - 1]}
\end{bmatrix} +
\begin{bmatrix}
b_1^{[l]} \\
\vdots \\
b_j^{[l]} \\
\vdots \\
b_{n^{[l]}}^{[l]}
\end{bmatrix},
\end{align*}\]

&lt;p&gt;이를 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[\vec{z}_{:, i}^{[l]} = \vec{W}^{[l]} \vec{a}_{:, i}^{[l - 1]} + \vec{b}^{[l]}\]

&lt;p&gt;vector space는 다음과 같이 정의됩니다 .\(\vec{z}_{:, i}^{[l]} \in \mathbb{R}^{ {n^{[l]}}} , \vec{W}^{[l]} \in \mathbb{R}^{n^{[l]} \times n^{[l - 1]}}  , \vec{b}^{[l]} \in \mathbb{R}^{ {n^{[l]}}} , \vec{a}_{:, i}^{[l - 1]} \in \mathbb{R}^{ {n^{[l - 1]}}}\)&lt;/p&gt;

&lt;p&gt;이는 1개의 training data에 대한  math expression입니다. 이를 이제 \(m\)개의 batch data의 size로 확장하여 vectorize를 해보겠습니다.&lt;/p&gt;

\[\begin{align}
\vec{Z}^{[l]} &amp;amp;=
\begin{bmatrix}
\vec{z}_{:, 1}^{[l]} &amp;amp; \dots &amp;amp; \vec{z}_{:, i}^{[l]} &amp;amp; \dots &amp;amp; \vec{z}_{:, m}^{[l]}
\end{bmatrix}  \\
&amp;amp;= \vec{W}^{[l]}
\begin{bmatrix}
\vec{a}_{:, 1}^{[l - 1]} &amp;amp; \dots &amp;amp; \vec{a}_{:, i}^{[l - 1]} &amp;amp; \dots &amp;amp; \vec{a}_{:, m}^{[l - 1]}
\end{bmatrix} +
\begin{bmatrix}
\vec{b}^{[l]} &amp;amp; \dots &amp;amp; \vec{b}^{[l]} &amp;amp; \dots &amp;amp; \vec{b}^{[l]}
\end{bmatrix} \notag \\
&amp;amp;= \vec{W}^{[l]} \vec{A}^{[l - 1]} + broadcast(\vec{b}^{[l]}), \notag \\
\end{align}\]

&lt;p&gt;vector space는 다음과 같이 정의됩니다 .\(\vec{Z}^{[l]} \in \mathbb{R}^{n^{[l]} \times m} , \vec{A}^{[l - 1]} \in \mathbb{R}^{n^{[l - 1]} \times m}\)&lt;/p&gt;

&lt;h3 id=&quot;al-의-vectorization&quot;&gt;\({A}^{[l]}\) 의 vectorization&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://onedrive.live.com/embed?resid=7E81BBCD99889380%217804&amp;amp;authkey=%21AF5RN-qLjiNzdI4&amp;amp;height=1121&amp;amp;width=1233&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 \(Z^{[l]}\) 을 activation function \(g^{[l]}\) 에 parameter 로 넘겨주는 그림입니다. \(Z^{[l]}\) 의 모든 node가 \({A}^{[l]}\)의 각 node에 전부 연결되어 있습니다. \(g^{[l]}\) 이 아직 확정이 되지 않았기에 그렇습니다. 보통은 RELU를 사용하게 되어 1 대 1 mapping 관계가 되지만, softmax를 사용하게 된다면 \(n^{[l]}\) 대 1 mapping 관계가 될 것입니다. notation은 &lt;a href=&quot;#term-definition&quot;&gt;Definition&lt;/a&gt;을 참조하시면됩니다.&lt;/p&gt;

&lt;p&gt;아래와 같은 식으로 표현할 수 있습니다.&lt;/p&gt;

\[\begin{equation}
a_{j, i}^{[l]} = g_j^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}). 
\end{equation}\]

&lt;p&gt;위 수식을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(2)&lt;/code&gt; 이라 하겠습니다.&lt;/p&gt;

&lt;p&gt;마찬가지로, sequential하게 하는것이 아닌 parallell 하게 진행되기 때문에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(2)&lt;/code&gt; 를 vectorize 해보도록 하겠습니다.&lt;/p&gt;

\[\begin{align*}
\begin{bmatrix}
a_{1, i}^{[l]} \\
\vdots \\
a_{j, i}^{[l]} \\
\vdots \\
a_{n^{[l]}, i}^{[l]}
\end{bmatrix} &amp;amp;=
\begin{bmatrix}
g_1^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}) \\
\vdots \\
g_j^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}) \\
\vdots \\
g_{n^{[l]}}^{[l]}(z_{1, i}^{[l]}, \dots, z_{j, i}^{[l]}, \dots, z_{n^{[l]}, i}^{[l]}) \\
\end{bmatrix}
\end{align*}\]

&lt;p&gt;이를 수식으로 표현하면&lt;/p&gt;

\[\vec{a}_{:, i}^{[l]} = \vec{g}^{[l]}(\vec{z}_{:, i}^{[l]})\]

&lt;p&gt;vector space는 다음과 같이 정의됩니다 .\(\vec{a}_{:, i}^{[l]} \in R^{n^{[l]}}\)&lt;/p&gt;

&lt;p&gt;위의 수식은  전체 activation 중 1개의 node를 의미합니다. 이를 전체 activation에 대해 확장해보도록 하겠습니다 .&lt;/p&gt;

\[\vec{A}^{[l]} =
\begin{bmatrix}
\vec{a}_{:, 1}^{[l]} &amp;amp; \dots &amp;amp; \vec{a}_{:, i}^{[l]} &amp;amp; \dots &amp;amp; \vec{a}_{:, m}^{[l]}
\end{bmatrix},\]

&lt;p&gt;vector space는 다음과 같이 정의됩니다. \(\vec{A}^{[l]} \in R^{n^{[l]} \times m}\)&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;임의의 layer의 vectorize 과정을 수학적으로 직접 도출해보았습니다. 아래와 같은 이유로 도움이 될 것이라 여겨집니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;neural net에 대한 이해도가 올라간다.
neural net의 작동 원리를 깊게 이해하면, 모델의 성능을 향상시키고 문제를 해결하는 데 도움이 됩니다. 일반항을 도출하면 neural net의 내부 작동 방식을 더 잘 이해할 수 있습니다&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;neural net이 어떻게 계산을 최적화 하는지 이해
vectorization 과정을 수식으로 도출하면, 어떻게 최적화할 수 있는지에 대한 통찰력을 얻을 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;문제 해결 능력 
벡터화 과정을 수식으로 도출하면, 모델이 예상대로 작동하지 않을 때 문제를 진단하고 해결하는 데 도움이 될 것입니다.. 이는 디버깅 및 최적화 과정에서 매우 유용합니다.\&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;커뮤니케이션 능력 향상
이러한 수식을 도출하고 이해하는 능력은 다른 엔지니어, 연구원, 이해관계자와의 커뮤니케이션에서 중요합니다. 이를 통해 복잡한 개념을 명확하게 전달하고, 팀의 협업을 촉진할 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;가장 기본이다.
feedforward network는 가장 기본적인 형태이기에 vectorize 과정을 수식으로 도출을 해본다면, 다른 neural net을 도출해내는 데에도 도움이 될 것입니다. 뿐만 아니라, 다른  neural net 모델들을 분해해보면 feedforward 가 사용되어지는 경우가 많습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://jonaslalin.com/2021/12/10/feedforward-neural-networks-part-1/&quot;&gt;feedforward-neural-networks-part-1/journalsim From Jonas Lalin &lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;&gt;wikiepdia&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Choi Woongjoon</name></author><category term="DLArchitecture" /><category term="Machine Learning" /><category term="deep learning" /><summary type="html">Mathematical view</summary></entry><entry><title type="html">The Phyiscal Layer</title><link href="http://localhost:4000/network/The-Physical-Layer/" rel="alternate" type="text/html" title="The Phyiscal Layer" /><published>2022-10-03T00:00:00+09:00</published><updated>2022-10-03T00:00:00+09:00</updated><id>http://localhost:4000/network/The-Physical-Layer</id><content type="html" xml:base="http://localhost:4000/network/The-Physical-Layer/">&lt;p&gt;예전 Post에서 TCP-IP Model에 대해 설명을 한 적이 있습니다. TCP-IP Model은 5개의 layer로 이루어져 있는데, 오늘은 가장 아래 layer인 Physical Layer Model에 대해서 설명해보도록 하겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;physical-layer&quot;&gt;Physical Layer?&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/192401951-1c3511fd-0043-4d22-84a4-ccca355c97fc.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Physical Layer란 여러 device에서 다른 Network로 Bit라는 정보를 보내게 됩니다. Bit는 computer가 이해할 수 있는 data 의 representation입니다. 0,1의 값을 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;구리 네트워크 케이블에 Modulation이라는 과정을 거쳐서 bit를 만들어 보내게 됩니다. Modulation이란 cable을 따라 움직이는 전하의 전압을 다르게 하는것을 의미합니다. 
&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/192905671-59927d83-99a7-4192-a317-c08583b6b058.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Computer Networking에서 이러한 Modulation을 line coding이라 부릅니다. 현대 networking에서는 10 기가비트 네트워크라는 단어가 많이 등장하는데 이는 100억개의 bit(0,1)을 각 cable마다 1초에 처리한다는 의미입니다.&lt;/p&gt;

&lt;h1 id=&quot;twisted-pair-cabling&quot;&gt;Twisted Pair Cabling&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/192906277-22635c2f-4c75-4f5f-9ba2-06b0fc052f38.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;twisted pair란 구리선이 꼬아져있음을 의미합니다. 꼬아져 있는 cable pair로 이루어져 있어서 twisted pair cable이라 합니다. 이렇게 cable을 꼬아두면 , 전자기간섭 , 즉 crosstalk을 줄여줍니다.&lt;br /&gt;
예를 들어, cat6 cable은 8개의 wire,즉 4 pair를 사용합니다. 전송기술에 따라서  몇개의 pair를 사용할 지 결정합니다.&lt;/p&gt;
&lt;h1 id=&quot;duplexing&quot;&gt;Duplexing&lt;/h1&gt;

&lt;p&gt;근대의 네트워킹에서 cable은 duplex communication이나 simplex communication을 사용합니다.  &lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/192907374-7139d5a2-fa5c-4ab9-9c73-eb429b4f7593.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;simplex는 정보를 한 방향으로만 전송하는 communication 방식을 의미합니다.그림과 같이 한 쪽의 device가 일방적으로 수신하거나 송신하는것을 볼 수 있습니다. 대표적인 예시로는 , 라디오나 TV가 있습니다. 방송국에서 전파를 송신하고 , 청자는 그 전파를 수신하기만 합니다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/193703834-b4a8bc16-a6b1-45cb-93ed-7761ef8dc4c8.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Duplex는 Client와 Server가 양방향으로 communication을 할 수 있는 방식입니다. 그렇다면 , 이것이 어떻게 가능한 것일까요?&lt;br /&gt;
cable은 여러개의 pair로 이루어져 있는데 ,각 pair를 한 방향으로만 미리 예약을 해두는 것입니다. 정확히 동시에 양방향 communication이 가능하다면 , 이를 full duplex라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/192907219-b76b3246-9525-473f-a3f5-52338320cef1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약에 Network에 문제가 생긴다면, Network는 link가 degrade 되고 , half-duplex communication을 한다고 보고합니다.&lt;br /&gt;
이 때, Network는 양방향으로 communication이 가능하지만, 한번에 한방향으로만 가능합니다.&lt;/p&gt;

&lt;h1 id=&quot;network-ports-and-panel&quot;&gt;Network Ports and panel&lt;/h1&gt;

&lt;p&gt;Twisted Pair network cable에는 끝에 Plug가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/193705135-31a0b36a-c130-4664-861c-a2d8071ecb6e.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가장 대중적인 plug는 RJ45 plug(Registered Jack 45)가 있습니다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/193705565-1e8ffa88-ec48-40a4-9dbb-c64140880ae7.png&quot; alt=&quot;image&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/193705655-9bafd997-c574-4eb7-ba11-c963cbcda4dc.png&quot; alt=&quot;image&quot; /&gt;&lt;br /&gt;
저번에 설명했던, Switch에는 여러개의 port가 있는반면, Server나 Desktop에는 1 개나 2개의 port가 달려있습니다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/193705381-c4cb44c5-1763-43dd-80cb-555e3f94757c.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모든 네트워크 port는 link led, activity led라는 2개의 port를 가지고 있습니다. LinkLed는 두 Device가 잘  연결되면 전원이 켜집니다. activityLed는 data가 활발하게 전송되면 ,  LED가 깜빡입니다. &lt;br /&gt;
요즘은, 컴퓨터 네트워크가 워낙 빨라서 ,Activity Led는 실제로 traffic이 있는지 없는지 외에는 의미하지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/193705887-78fbb6cf-3513-46a8-a44d-f4fd93120f73.png&quot; alt=&quot;image&quot; /&gt;
가끔씩, 책상 아래에 network port가 붙어있는 경우가 있습니다. 이 포트들은 네트워크 케이블을 통해서 , 벽을 넘어가서 patch-panel에 도달하게 됩니다. patch-panel에는 많은 port들이 잇지만, 그저 port의 container일 뿐입니다. 이 patch panel로 부터 나온 cable은 switch나 router나 computer로 향하게 됩니다.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;&amp;quot;https://www.coursera.org/learn/computer-networking/lecture/Nihjd/moving-bits-across-the-wire&amp;quot;&quot;&gt;Google-IT-support&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;&amp;quot;https://en.wikipedia.org/wiki/Twisted_pair&amp;quot;&quot;&gt;Twisted-pairwiki&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;&amp;quot;https://en.wikipedia.org/wiki/Simplex_communication&amp;quot;&quot;&gt;Simplex&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;&amp;quot;https://en.wikipedia.org/wiki/Duplex_(telecommunications)&amp;quot;&quot;&gt;Duplex&lt;/a&gt;&lt;/p&gt;</content><author><name>Choi Woongjoon</name></author><category term="Network" /><category term="SoftwareEngineering" /><category term="Network" /><category term="TCPIP" /><summary type="html">TCP 5 layer model-phyiscal</summary></entry><entry><title type="html">Pytorch data api : Multiprocessing</title><link href="http://localhost:4000/pytorch/Single-and-MultiProcessing/" rel="alternate" type="text/html" title="Pytorch data api : Multiprocessing" /><published>2022-09-16T00:00:00+09:00</published><updated>2022-09-16T00:00:00+09:00</updated><id>http://localhost:4000/pytorch/Single-and-MultiProcessing</id><content type="html" xml:base="http://localhost:4000/pytorch/Single-and-MultiProcessing/">&lt;p&gt;Pytorch에서 DataLoading을 할 때 DataLoader라는 class를 사용하는데 , 이 때 Data를 단일 Process내에서 loading 할수도 있고, parallelize해서 loading을 할 수도 있다.&lt;/p&gt;

&lt;h1 id=&quot;default-option&quot;&gt;Default Option&lt;/h1&gt;
&lt;p&gt;DataLoader는 Single Process data loading이 default option입니다 .&lt;/p&gt;

&lt;p&gt;Python Process는 GIL(Global Interpretatoin Lock)이 Python Code를 thread로 parallelize하는 것을 막습니다. 따라서, data loading에서 computation을 1개의 process가 blocking 하는것을 방지하기 위해서 ,Pytorch는 ‘num_workers’ 라는 argument를 양의 정수로 설정하여 multi-process data loading으로 쉽게 전환 시킵니다.&lt;/p&gt;

&lt;h1 id=&quot;single-processing&quot;&gt;Single Processing&lt;/h1&gt;

&lt;p&gt;data fetching이 같은 process내에서 이루어집니다. 즉, 1개의 process가 computing을 block하는 상태입니다. 
이 방법은 resource가 제한이 되거나 , 전체 memory 에 data를 올릴 수 있을만큼 작다면 , 선호되는 방법입니다.&lt;br /&gt;
또한, 이 방법의 장점으로는 error tracing이 쉽다는 것입니다. 이에 대해서는, Multi Processing에서 자세히 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;multi-processing&quot;&gt;Multi Processing&lt;/h1&gt;

&lt;p&gt;Dataloader에서는 num_workers라는 argument를 설정할 수 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;‘num_workers’ = 0
    &lt;ul&gt;
      &lt;li&gt;data loading이 main process에서 이루어집니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;‘num_workers’ &amp;gt; 0
    &lt;ul&gt;
      &lt;li&gt;multi-process data loading 이 활성화되고 , 지정한 숫자만큼의 worker process가 생성이 됩니다 . 이 subprocess들이 data loading에 사용됩니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;data를 loading할 때 에는 , enumerate(dataloader)를 call 하여 , 매번 DataLoader의 iterator를 생성합니다. 여기에서 , `num_workers’ argument에 지정한 개수 만큼의 worker_process가 생성이 됩니다. 그리고 , ‘dataset’ , ‘collate_fn’, ‘worker_init_fn’이 각  worker에 전달이 됩니다. worker가  initialize 된 후 ,   data가 fetch 되어집니다.&lt;/p&gt;

&lt;p&gt;즉 , Internal I/O , transformation(‘collate_fn’ 포함)이  dataset access와 함께 , worker_process에서 실행되어짐을 의미합니다.&lt;/p&gt;

&lt;h2 id=&quot;iterating-dataloader&quot;&gt;Iterating DataLoader&lt;/h2&gt;

&lt;p&gt;실제로 , Data를 여러개의 worker에서 fetching 할 때에는 , torch.utils.data.get_worekr_info()라는 method를 사용합니다. &lt;br /&gt;
이 function은 아래와 같은 항목을 return합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;worker_id&lt;/li&gt;
  &lt;li&gt;dataset replica&lt;/li&gt;
  &lt;li&gt;initial seed&lt;/li&gt;
  &lt;li&gt;.etc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;main_process에서는 None을 return 하게 됩니다. Pytorch 개발자들은 이 function을 dataset code나 worker_init_fn에서 사용하여 code가 worker_process에서 실행중인지 아닌지를 판별하여 개별적인 dataset_replica를 configure합니다.&lt;/p&gt;

&lt;p&gt;특히, data sharding에 도움이 된다고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;map-style&quot;&gt;Map-style&lt;/h3&gt;

&lt;p&gt;map-style dataset에서 여러 subprocess들을 만들게 되면 , sampler를 사용해서 indicies를 만들게 되고 이를 각 worker에 전달하게 됩니다 . sampler에서의 shuffling 은 main process에서 수행이 되어집니다. &lt;br /&gt;
즉, main process는 data loading에서 shuffle된 indicies를 worker에 할당하여 data를 loading하도록 합니다. &lt;br /&gt;
아래의 코드 예시를 보도록 하겠습니다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyMapDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyMapDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;this example code only works with end&amp;gt; = start&quot;&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; 
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__getitem__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;worekr_id : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; data : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__len__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;map_ds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyMapDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_ds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# worekr_id : 0 data : 3
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# worekr_id : 1 data : 4
# worekr_id : 0 data : 5
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# worekr_id : 1 data : 6
# worekr_id : 0 data : 7
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# worekr_id : 1 data : 8
# worekr_id : 0 data : 9
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# worekr_id : 1 data : 10
# worekr_id : 0 data : 11
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# worekr_id : 1 data : 12
# worekr_id : 0 data : 13
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# worekr_id : 1 data : 14
# worekr_id : 0 data : 15
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# worekr_id : 1 data : 16
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# worekr_id : 1 data : 18
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#  ... 
# worekr_id : 0 data : 93
# worekr_id : 1 data : 96
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# worekr_id : 0 data : 95
# worekr_id : 1 data : 98
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# worekr_id : 0 data : 97
# worekr_id : 0 data : 99
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# [tensor([3]), tensor([4]), tensor([5]), tensor([6]), tensor([7]), tensor([8]), tensor([9]), tensor([10]), tensor([11]), tensor([12]), tensor([13]), tensor([14]), tensor([15]), tensor([16]), tensor([17]), tensor([18]), tensor([19]), tensor([20]), tensor([21]), tensor([22]), tensor([23]), tensor([24]), tensor([25]), tensor([26]), tensor([27]), tensor([28]), tensor([29]), tensor([30]), tensor([31]), tensor([32]), tensor([33]), tensor([34]), tensor([35]), tensor([36]), tensor([37]), tensor([38]), tensor([39]), tensor([40]), tensor([41]), tensor([42]), tensor([43]), tensor([44]), tensor([45]), tensor([46]), tensor([47]), tensor([48]), tensor([49]), tensor([50]), tensor([51]), tensor([52]), tensor([53]), tensor([54]), tensor([55]), tensor([56]), tensor([57]), tensor([58]), tensor([59]), tensor([60]), tensor([61]), tensor([62]), tensor([63]), tensor([64]), tensor([65]), tensor([66]), tensor([67]), tensor([68]), tensor([69]), tensor([70]), tensor([71]), tensor([72]), tensor([73]), tensor([74]), tensor([75]), tensor([76]), tensor([77]), tensor([78]), tensor([79]), tensor([80]), tensor([81]), tensor([82]), tensor([83]), tensor([84]), tensor([85]), tensor([86]), tensor([87]), tensor([88]), tensor([89]), tensor([90]), tensor([91]), tensor([92]), tensor([93]), tensor([94]), tensor([95]), tensor([96]), tensor([97]), tensor([98]), tensor([99])]
&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;보시는 바와 같이 각 indicies들이 worker에 전달되어 data를 loading함을 알 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;iterative-style&quot;&gt;Iterative-style&lt;/h3&gt;
&lt;p&gt;Iterable sytle의 dataset을 worker를 이용해서 loading할 때에는 data 중복을 주의해야 합니다. &lt;br /&gt;
Iterable style의 dataset을 loading할 때 subprocess들을 만드는 경우 , 각 worker들이 dataset object의 replica를 얻게 됩니다. 그 다음에, 각 worker들이 dataset object를 iterating 함으로 써 , data의 중복이 발생하게 됩니다. 아래의 예시코드에서 확인해보도록 하겠습니다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyIterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyIterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;this example code only works with end &amp;gt;=start&quot;&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__iter__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyIterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# range(3, 100)
# WorkerInfo(id=0, num_workers=2, seed=4911920692807402111, dataset=&amp;lt;__main__.MyIterableDataset object at 0x7f5d2ef69910&amp;gt;)range(3, 100)
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# WorkerInfo(id=1, num_workers=2, seed=4911920692807402112, dataset=&amp;lt;__main__.MyIterableDataset object at 0x7f5d2ef69910&amp;gt;)
# [tensor([3]), tensor([3]), tensor([4]), tensor([4]), tensor([5]), tensor([5]), tensor([6]), tensor([6]), tensor([7]), tensor([7]), tensor([8]), tensor([8]), tensor([9]), tensor([9]), tensor([10]), tensor([10]), tensor([11]), tensor([11]), tensor([12]), tensor([12]), tensor([13]), tensor([13]), tensor([14]), tensor([14]), tensor([15]), tensor([15]), tensor([16]), tensor([16]), tensor([17]), tensor([17]), tensor([18]), tensor([18]), tensor([19]), tensor([19]), tensor([20]), tensor([20]), tensor([21]), tensor([21]), tensor([22]), tensor([22]), tensor([23]), tensor([23]), tensor([24]), tensor([24]), tensor([25]), tensor([25]), tensor([26]), tensor([26]), tensor([27]), tensor([27]), tensor([28]), tensor([28]), tensor([29]), tensor([29]), tensor([30]), tensor([30]), tensor([31]), tensor([31]), tensor([32]), tensor([32]), tensor([33]), tensor([33]), tensor([34]), tensor([34]), tensor([35]), tensor([35]), tensor([36]), tensor([36]), tensor([37]), tensor([37]), tensor([38]), tensor([38]), tensor([39]), tensor([39]), tensor([40]), tensor([40]), tensor([41]), tensor([41]), tensor([42]), tensor([42]), tensor([43]), tensor([43]), tensor([44]), tensor([44]), tensor([45]), tensor([45]), tensor([46]), tensor([46]), tensor([47]), tensor([47]), tensor([48]), tensor([48]), tensor([49]), tensor([49]), tensor([50]), tensor([50]), tensor([51]), tensor([51]), tensor([52]), tensor([52]), tensor([53]), tensor([53]), tensor([54]), tensor([54]), tensor([55]), tensor([55]), tensor([56]), tensor([56]), tensor([57]), tensor([57]), tensor([58]), tensor([58]), tensor([59]), tensor([59]), tensor([60]), tensor([60]), tensor([61]), tensor([61]), tensor([62]), tensor([62]), tensor([63]), tensor([63]), tensor([64]), tensor([64]), tensor([65]), tensor([65]), tensor([66]), tensor([66]), tensor([67]), tensor([67]), tensor([68]), tensor([68]), tensor([69]), tensor([69]), tensor([70]), tensor([70]), tensor([71]), tensor([71]), tensor([72]), tensor([72]), tensor([73]), tensor([73]), tensor([74]), tensor([74]), tensor([75]), tensor([75]), tensor([76]), tensor([76]), tensor([77]), tensor([77]), tensor([78]), tensor([78]), tensor([79]), tensor([79]), tensor([80]), tensor([80]), tensor([81]), tensor([81]), tensor([82]), tensor([82]), tensor([83]), tensor([83]), tensor([84]), tensor([84]), tensor([85]), tensor([85]), tensor([86]), tensor([86]), tensor([87]), tensor([87]), tensor([88]), tensor([88]), tensor([89]), tensor([89]), tensor([90]), tensor([90]), tensor([91]), tensor([91]), tensor([92]), tensor([92]), tensor([93]), tensor([93]), tensor([94]), tensor([94]), tensor([95]), tensor([95]), tensor([96]), tensor([96]), tensor([97]), tensor([97]), tensor([98]), tensor([98]), tensor([99]), tensor([99])]
&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;dataset의 replica가 각 worker에 전달되어 중복되게 fetching함을 알 수 있습니다. &lt;br /&gt;
이를 해결하기 위해서는 여러가지 방법이 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;using-get_worker_info&quot;&gt;Using get_worker_info()&lt;/h4&gt;

&lt;p&gt;위에서 언급했듯이 , get_worker_info() 는 main_process의 경우 None을 return하고 , subprocess의 경우에는 id,replica ,seed 등등을 return합니다. worker_id를 이용해서 각 worker마다 fetching을 configuration을 할 수 있습니다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyIterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyIterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;this example code only works with end &amp;gt;= start&quot;&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__iter__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;iter_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;iter_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;per_worker&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ceil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;
                             &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;iter_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per_worker&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;iter_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per_worker&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'worker_id : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; iter_start : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter_start&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;  iter_end : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter_end&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyIterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 


&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# worker_id : 0 
#  iter_start : 3  iter_end : 35
# worker_id : 1 
#  iter_start : 35  iter_end : 67
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# worker_id : 2 
#  iter_start : 67  iter_end : 99
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# [tensor([3]), tensor([35]), tensor([67]), tensor([4]), tensor([36]), tensor([68]), tensor([5]), tensor([37]), tensor([69]), tensor([6]), tensor([38]), tensor([70]), tensor([7]), tensor([39]), tensor([71]), tensor([8]), tensor([40]), tensor([72]), tensor([9]), tensor([41]), tensor([73]), tensor([10]), tensor([42]), tensor([74]), tensor([11]), tensor([43]), tensor([75]), tensor([12]), tensor([44]), tensor([76]), tensor([13]), tensor([45]), tensor([77]), tensor([14]), tensor([46]), tensor([78]), tensor([15]), tensor([47]), tensor([79]), tensor([16]), tensor([48]), tensor([80]), tensor([17]), tensor([49]), tensor([81]), tensor([18]), tensor([50]), tensor([82]), tensor([19]), tensor([51]), tensor([83]), tensor([20]), tensor([52]), tensor([84]), tensor([21]), tensor([53]), tensor([85]), tensor([22]), tensor([54]), tensor([86]), tensor([23]), tensor([55]), tensor([87]), tensor([24]), tensor([56]), tensor([88]), tensor([25]), tensor([57]), tensor([89]), tensor([26]), tensor([58]), tensor([90]), tensor([27]), tensor([59]), tensor([91]), tensor([28]), tensor([60]), tensor([92]), tensor([29]), tensor([61]), tensor([93]), tensor([30]), tensor([62]), tensor([94]), tensor([31]), tensor([63]), tensor([95]), tensor([32]), tensor([64]), tensor([96]), tensor([33]), tensor([65]), tensor([97]), tensor([34]), tensor([66]), tensor([98])]
&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위의 code에서 iter method는 subprocess가 설정되면 , num_workers만큼 data를 나누어서 worker_id를 기준으로 fetching하도록 configuration 합니다.&lt;/p&gt;

&lt;h4 id=&quot;using-worker_init_fnworker_id&quot;&gt;Using worker_init_fn(worker_id)&lt;/h4&gt;

&lt;p&gt;pytorch dataloader의 argument에는 worker_init_fn을 설정할 수 있습니다. worker_init_fn은 worker_id를 argument로 받아서, 각 dataset의 replica를 개별적으로 설정합니다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyIterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyIterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;this example code only works with end &amp;gt;=start&quot;&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__iter__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'worker_id : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; iter_start : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;  iter_end : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;worker_init_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;overall_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;overall_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;per_worker&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ceil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;overall_end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;overall_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;
                     &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;overall_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per_worker&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per_worker&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;overall_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# print(f'worker_id : {worker_id} , worker_start : {dataset.start}  ,  worekr_end : {dataset.end}')
&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyIterableDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 


&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker_init_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker_init_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
#   cpuset_checked))
# worker_id : 0 
#  iter_start : 3  iter_end : 12
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# worker_id : 1 
#  iter_start : 12  iter_end : 21
# worker_id : 2 
#  iter_start : 21  iter_end : 30
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# worker_id : 3 
#  iter_start : 30  iter_end : 39
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# worker_id : 4 
#  iter_start : 39  iter_end : 48
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# worker_id : 5 
#  iter_start : 48  iter_end : 57
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# worker_id : 6 
#  iter_start : 57  iter_end : 66
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# worker_id : 7 
#  iter_start : 66  iter_end : 75
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# worker_id : 8 
#  iter_start : 75  iter_end : 84
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# worker_id : 9 
#  iter_start : 84  iter_end : 93
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# [tensor([3]), tensor([12]), tensor([21]), tensor([30]), tensor([39]), tensor([48]), tensor([57]), tensor([66]), tensor([75]), tensor([84]), tensor([4]), tensor([13]), tensor([22]), tensor([31]), tensor([40]), tensor([49]), tensor([58]), tensor([67]), tensor([76]), tensor([85]), tensor([5]), tensor([14]), tensor([23]), tensor([32]), tensor([41]), tensor([50]), tensor([59]), tensor([68]), tensor([77]), tensor([86]), tensor([6]), tensor([15]), tensor([24]), tensor([33]), tensor([42]), tensor([51]), tensor([60]), tensor([69]), tensor([78]), tensor([87]), tensor([7]), tensor([16]), tensor([25]), tensor([34]), tensor([43]), tensor([52]), tensor([61]), tensor([70]), tensor([79]), tensor([88]), tensor([8]), tensor([17]), tensor([26]), tensor([35]), tensor([44]), tensor([53]), tensor([62]), tensor([71]), tensor([80]), tensor([89]), tensor([9]), tensor([18]), tensor([27]), tensor([36]), tensor([45]), tensor([54]), tensor([63]), tensor([72]), tensor([81]), tensor([90]), tensor([10]), tensor([19]), tensor([28]), tensor([37]), tensor([46]), tensor([55]), tensor([64]), tensor([73]), tensor([82]), tensor([91]), tensor([11]), tensor([20]), tensor([29]), tensor([38]), tensor([47]), tensor([56]), tensor([65]), tensor([74]), tensor([83]), tensor([92])]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;warning&quot;&gt;Warning&lt;/h3&gt;

&lt;p&gt;Pytorch Docs에서는 CUDA Tensor를 multi-processing loading 에서 return하는 것을 추천하지 않는다고 한다.  CUDA Tensor를 공유하거나 CUDA를 사용하는 대신에 , automatic memory pinning(pin_memory = True)을 이용해서  사용하는 것을 추천한다고 합니다. 이는 CUDA가 사용가능한 GPO로 빠른 data 전송을 하게 한다고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;platform-specific&quot;&gt;Platform Specific&lt;/h2&gt;

&lt;p&gt;Python Multiprocessing을 사용하게 되면, OS에 따라서 , worker launch behavior가 달라집니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;UNIX
    &lt;ul&gt;
      &lt;li&gt;fork()가  multiprocessing을 시작하는 default method입니다. fork()를 사용하면 , child worker들은 복제된 address space를 통해 dataset과 Python functions에 직접 access 할 수 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Windows, MAC
    &lt;ul&gt;
      &lt;li&gt;spawn()이 multiprocessing을 시작하는 default method입니다. spawn()을 사용해서 , 다른 interpereter들이 실행되면서 , main script를 실행합니다. 그 다음에 , pickle serialization을 통해 dataset, collate_fn , 그리고 다른 argument를 serialization 하고 , internal worker function을 실행합니다. serialization을 사용한다는건 multiprocess data load를 사용하는동안에 Windows와 호환이 되는지 확인하는 2단계를 실행해야 함을 의미합니다.
        &lt;ol&gt;
          &lt;li&gt;main script code를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;if __name__  =='__main__'&lt;/code&gt; block으로 둘러쌉니다.
            &lt;ul&gt;
              &lt;li&gt;why? 각 worker process가 실행될 때 , 다시는 main script code가 실행되지 않도록 하기 위해서입니다. main script code에 Dataset,Datalodaer instance 생성 코드를 포함시켜 , worker에서 다시 실행되지 않도록 합니다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;custom collate_fn ,worker_init_fn , 그리고 dataset code가 top level definitions에서 ,즉 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__main__&lt;/code&gt; 을 check하는 code 바깥에서  define되도록 합니다.
            &lt;ul&gt;
              &lt;li&gt;why? fucntions들이 bytecode가 아닌 reference로써 pickled 되기 때문입니다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;randomness-in-multiprocessing-data-loading&quot;&gt;Randomness in multiprocessing data loading&lt;/h2&gt;

&lt;p&gt;각 worker는 seed를  (base_seed + worker_id)로 설정합니다. base_seed는 main_process에 의해서 생성이 되는데 , 이 때 RNG(Random Number Generator) 나 지정한 generator를 이용하게 됩니다. 하지만, 다른 라이브러리의 seed가 중복이 될 수 있습니다. 따라서 ,  worker 가 initialized 될 때 , 각 worker가 동일한 random number를 return할 수 있습니다. &lt;br /&gt;
worker_init_fn에서 torch.utils.data.get_worker_info().seed 나 torch.initial.seed를 사용하면 각 worker에 대한 Pytorch seed set에 access하고 , 이를 사용하여  , data 를 loading하기 전에 다른 라이브러리로 seed를 전달할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;issues&quot;&gt;Issues&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Problem
    &lt;ul&gt;
      &lt;li&gt;iteration을 몇 번 반복하고 나면 , loader worker process가 parent process에 있는 모든 Python Objects(worker process에서 access가능)에 대해 같은 양의 CPU Memory를 점유하고 있습니다.만약에 , Dataset이 엄청나게 큰 data(ex.매우 큰 filename lsit)를 포함하거나 수많은 worker를 사용한다면 문제가 발생할 것입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Why?
    &lt;ul&gt;
      &lt;li&gt;임의의 Python Objects를  shared memmory에 저장하는 것은 copy-on-write problem을 발생시킵니다. 이 object들을 read할 때마다 , reference count를 증가시킵니다. reference count의 변화로 인해 fork된 python process의 copy-on-acess problem이 발생하게 되는것입니다. (Memory-leak 문제가 아닙니다.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sol
    &lt;ul&gt;
      &lt;li&gt;기본적인 Python Objects(list,dict) 대신에 pandas,numpy,pyarrow 같은 objects를 사용합니다. 이들은 reference count가 1입니다.&lt;/li&gt;
      &lt;li&gt;String을 저장할 때에는 , ASCII code로 numpy array를 사용하여 저장할 수 있습니다. 아니면, ByteCode나 Custom Datatype을 사용할 수 있습니다.
        &lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;&amp;quot;https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading&amp;quot;&quot;&gt;Pytorch-data-docs&lt;/a&gt;&lt;/p&gt;</content><author><name>Choi Woongjoon</name></author><category term="Pytorch" /><category term="Machine Learning" /><category term="Pytorch" /><category term="deep learning" /><summary type="html">Pytorch multiprocess</summary></entry><entry><title type="html">Numerics</title><link href="http://localhost:4000/python/Numeric/" rel="alternate" type="text/html" title="Numerics" /><published>2022-09-12T00:00:00+09:00</published><updated>2022-09-12T00:00:00+09:00</updated><id>http://localhost:4000/python/Numeric</id><content type="html" xml:base="http://localhost:4000/python/Numeric/">&lt;p&gt;Numeric Types라는 Built-in Objects를 잘 이용하기 위해서는 이 Built-in Objects에 어떤 Literals들이 있고 , 어떤 operator들을 제공하는지 알아야합니다. 이것들에 대해서 알아보도록 하겠습니다 .&lt;/p&gt;

&lt;h1 id=&quot;numeric-literals&quot;&gt;Numeric Literals&lt;/h1&gt;
&lt;h2 id=&quot;integers&quot;&gt;Integers&lt;/h2&gt;
&lt;p&gt;Integer 타입은 decimal digits으로 쓰여진 string 입니다. 
예시는 아래와 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#  24, 0 ,-123234556
&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;python-3x-vs-python-2x&quot;&gt;Python 3.x vs Python 2.x&lt;/h3&gt;
&lt;p&gt;Python의 Integer는 precision에 제한이 없습니다. 
하지만 ,&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Python 3.x 버전과 2.X버전에서의 Integer는 큰 차이점이 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2.x 버전에서는 Integer는 normal type과 long type으로 나뉩니다. normal integer는 32bit로 표현할 수 있는 decimal digit을 표현합니다. 하지만, long type은 precision에 제한이 없고 , decimal digit 끝이 l 혹은 L로 끝나게 됩니다. 허용된 bit인 32를 넘게되면, 자동으로 long integer로 변환이 됩니다.Programmer가 ‘L’을 type할 필요가 없습니다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mi&quot;&gt;2147689795&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;il&quot;&gt;2147689795L&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;하지만, 3.x버전에서는 Integer는 normal type과 long type이 병합되었습니다. 따라서, integer는 더이상 l 이나 L이 끝에 출력도지 않습니다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mi&quot;&gt;2147689795&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2147689795&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;hexbinaryoct&quot;&gt;Hex,Binary,Oct&lt;/h3&gt;

&lt;p&gt;여태까지, 위에서 본 Integer는 전부 decimal digits로 코딩된 Integer였습니다. Integer는 decimal 뿐만 아니라, hexadecimal, octal, binary로 코딩될 수 있습니다. &lt;br /&gt;
HexaDecimal은 (0x or 0X)(A-F and 0-9) 의 format으로 coding이 되어집니다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0xAF&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 175
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0XAF&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;말 그대로 코딩이 될 뿐 Integer이므로 출력은 Integer로 됩니다.&lt;/p&gt;

&lt;p&gt;Octal은  (0o or 0O)(0-7) 의 format으로 coding이 되어집니다.&lt;br /&gt;
2.x 버전에서는 0(0-7)의 format이였지만, decimal representation과 햇갈려서 3.x버전에서는 변경되었습니다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mo&quot;&gt;0o10&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 8
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mo&quot;&gt;0O10&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Binary는 (0b or 0B)(0-1) 의 format으로 coding이 되어집니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mb&quot;&gt;0b11&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 3
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mb&quot;&gt;0B11&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;create-integers-using-built-in-calls&quot;&gt;Create Integers Using Built-in Calls&lt;/h4&gt;

&lt;p&gt;위에서 사용되었던 Integer의 format은 Built-in method로 생성할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;hex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 0x10
# &amp;lt;class 'str'&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;oct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 0o10
# &amp;lt;class 'str'&amp;gt;
&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 0b10
# &amp;lt;class 'str'&amp;gt;
&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;hex,oct,bin 은 Integer를 argument로 받아 해당되는 representation을 string으로 return함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;literal expression으로 Integer object를 생성할 수 있지만 , built-in method로도 Integer Objects를 생성할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; type of a : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; type of a : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; type of a : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;10&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;d : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; type of a : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;e : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; type of a : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;k : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; type of a : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# a : 10
#  type of a : &amp;lt;class 'int'&amp;gt;
# b : 2
#  type of a : &amp;lt;class 'int'&amp;gt;
# c : 8
#  type of a : &amp;lt;class 'int'&amp;gt;
# d : 16
#  type of a : &amp;lt;class 'int'&amp;gt;
# e : 3
#  type of a : &amp;lt;class 'int'&amp;gt;
# k : 6
#  type of a : &amp;lt;class 'int'&amp;gt;
&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;int method는 int(str,base)  or int(digit) 로 정의되어 있습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;int(str,base)
    &lt;ul&gt;
      &lt;li&gt;base를 통해서 representation을 확인후 , 그 에 해당하는 integer objects를  return합니다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;int(digit)
    &lt;ul&gt;
      &lt;li&gt;digit에 해당하는 intger object를 return합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;floating-numbers&quot;&gt;Floating Numbers&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;floatnumber   ::=  pointfloat | exponentfloat
pointfloat    ::=  [digitpart] fraction | digitpart &quot;.&quot;
exponentfloat ::=  (digitpart | pointfloat) exponent
digitpart     ::=  digit ([&quot;_&quot;] digit)*
fraction      ::=  &quot;.&quot; digitpart
exponent      ::=  (&quot;e&quot; | &quot;E&quot;) [&quot;+&quot; | &quot;-&quot;] digitpart

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;floating literal의 lexical definition은 위와 같습니다.floatnumber는 pointfloat , exponentfloat 2가지의 literal로 구성되어집니다.  pointfloat , 즉 decimal 인 부분과 ‘.’으로만 이루어진 floatnumber입니다. exponentfloat은 추가적으로 exponent인 부분도 포함한다는 뜻입니다.&lt;/p&gt;

&lt;p&gt;Python에서 float literal를 사용하게 되면 , 이를 float object로 만들고  , floating object가 expression에서 사용되어 질 때 , floating-point math를 사용합니다.  floating object의 예시는 아래와 같습니다 .&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.15e-10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0e0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 3.14
# 10.0
# 0.0001
# 1e+100
# 3.15e-10
# 0.0
&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Floating-point number는 standard C python에서는 C의 “doubles”로 구현이 되어있습니다. 따라서, C compiler가 double에 제공하는 만큼의 precision을 얻게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;complex-numbers&quot;&gt;Complex Numbers&lt;/h2&gt;

&lt;p&gt;복소수는 실수와 허수를 갖는 숫자입니다.&lt;/p&gt;

&lt;p&gt;허수의 lexical definition은 아래와 같습니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;imagnumber ::=  (floatnumber | digitpart) (&quot;j&quot; | &quot;J&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;복소수에서 실수부는 optional이므로 생략하여도 괜찮습니다. &lt;br /&gt;
복소수의 예시는 아래와 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.15j&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;10.j&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;10j&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;001j&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e100j&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.14e-10j&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.14_15_93j&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# (3+5j)
# &amp;lt;class 'complex'&amp;gt;
# 3.15j
# &amp;lt;class 'complex'&amp;gt;
# 10j
# &amp;lt;class 'complex'&amp;gt;
# 10j
# &amp;lt;class 'complex'&amp;gt;
# 0.001j
# &amp;lt;class 'complex'&amp;gt;
# 1e+100j
# &amp;lt;class 'complex'&amp;gt;
# 3.14e-10j
# &amp;lt;class 'complex'&amp;gt;
# 3.141593j
# &amp;lt;class 'complex'&amp;gt;
&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;예를들어 ,3 + 5j 는 실수부 3, 허수부 5j를 가진 complex number입니다. 나머지는 , 허수만을 가진 complex number입니다.&lt;/p&gt;

&lt;h1 id=&quot;handling-numeric-types-using-built-in-tools&quot;&gt;Handling Numeric Types Using Built-in tools&lt;/h1&gt;

&lt;p&gt;Numeric Types object를 정의를 했다면,이 objects에 대한 method 나 operator를 정의하여 Numeric type objects를 다룰 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;expression-operator&quot;&gt;Expression Operator&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;&amp;quot;https://docs.python.org/ko/3/reference/expressions.html#operator-precedence&amp;quot;&quot;&gt;python-operator-list-and-precedence&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Python Operator의 목록과 우선순위는 documents site의 link를 첨부하도록 하겠습니다.&lt;br /&gt;
 Documents에서의 table에서 위에 있을수록 precedence가 높고 , 같은 precedence라면 왼쪽에서 오른쪽으로 expression이 진행이 됩니다.&lt;/p&gt;

&lt;p&gt;Python Expression Operator에서는 알아야 할 몇가지 특징이 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;괄호에 둘러쌓인 expression을 먼저 수행합니다.&lt;/li&gt;
  &lt;li&gt;서로 다른 type의 operands에 대해 expression을 수행할 때 , 더 복잡한 type의 operands로 type을 변환시킵니다. 그 다음에 , math operation을 수행합니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.14&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# &amp;lt;class 'float'&amp;gt;
&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;물론 , 명시적으로 type을 변환할 수 있습니다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.1425&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# &amp;lt;class 'int'&amp;gt;
# 3
# &amp;lt;class 'float'&amp;gt;
#3.0 
&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;즉, 자동적으로 type을 conversion 하기 때문에 , 명시적으로 type을 변환할 필요가 없습니다 .&lt;/p&gt;

&lt;h2 id=&quot;built-in-math-functions-and-utitlity-modules&quot;&gt;Built-in math functions and utitlity modules&lt;/h2&gt;

&lt;p&gt;Built-in math functions에는 pow , trunc,round , int 등등이 있고 , utility modules에는 random, math 등등이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;&amp;quot;https://docs.python.org/ko/3/library/numeric.html&amp;quot;&quot;&gt;Python-math-moduels&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;math-modules에 대한 document link를 첨부하도록 하겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;&amp;quot;https://docs.python.org/ko/3/library/stdtypes.html#numeric-types-int-float-complex&amp;quot;&quot;&gt;Python-Types-docs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;&amp;quot;https://docs.python.org/ko/3/reference/lexical_analysis.html#floating&amp;quot;&quot;&gt;Python-Lexical-analysis&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Learning-Python&lt;/p&gt;</content><author><name>Choi Woongjoon</name></author><category term="Python" /><category term="Python" /><category term="Object" /><category term="Builtin" /><summary type="html">Python Numerics</summary></entry><entry><title type="html">Polynomial Regression</title><link href="http://localhost:4000/standfordml/polynominal-regerssion/" rel="alternate" type="text/html" title="Polynomial Regression" /><published>2022-06-02T00:00:00+09:00</published><updated>2022-06-02T00:00:00+09:00</updated><id>http://localhost:4000/standfordml/polynominal-regerssion</id><content type="html" xml:base="http://localhost:4000/standfordml/polynominal-regerssion/">&lt;h1 id=&quot;polynomial-regression-in-multiple-features&quot;&gt;Polynomial Regression in Multiple Features&lt;/h1&gt;

&lt;p&gt;Linear Regression 모델을 1개의 독립변수 x와 1개의 의존변수 y로 나타내는 것을 Simple Linear Regression이라 한다. 이를 , 수식으로 표현하면&lt;br /&gt;
\(y = \theta_{1} x + \theta_{0}\)
로 표현할 수 있다. 이는 단순하게 x,y 축을 가진 그래프로 표현할 수 있기에 쉽게 이해할 수 있습니다. 하지만, 의존변수 y를 예측해야하는 Linear Regression 모델을 만들때 , 독립변수는 1개가 아닐 수 있습니다. 이때 , 사용하는 LInear Regerssion 모델은 다른 equation입니다.&lt;/p&gt;

&lt;h2 id=&quot;what-is-polynomial-regression&quot;&gt;What is Polynomial Regression&lt;/h2&gt;

&lt;p&gt;Polynomial Regression이란 독립변수가 1개가 아닌 여러개인 경우의 Linear Regression 모델입니다.&lt;/p&gt;

&lt;p&gt;이를 식으로 표현하면  아래와 같습니다.&lt;/p&gt;

\[\begin{align}
y &amp;amp;= \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{n}x_{n} \\
y &amp;amp;= \sum_{i=1}^{n} \theta_{i}x_{i} + \theta_{0} \\
\end{align}\]

&lt;p&gt;여기서 주의할 점은 모든 feature를사용하는 것이 아니라 , 필요한 만큼의 feature를 사용한다는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/170036215-31ef848c-168f-4f1d-9f17-e7c39509d458.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예를 들어, 위와 같이 데이터가 주어진 경우에는 feature를 1개만 사용하면 충분합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/170036527-36e28dc6-2dea-4dab-8e9e-affe47055672.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 데이터가 주어진 경우 , feature를 2개 (x, x*x) 를 사용하게 된다면 , 빨간색과 같은  model을 얻게 될 것입니다. 이는 데이터를 잘 표현하지 못하는 model입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/170037696-19d8cc0f-6b60-406e-b677-682a83f1e860.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약에 , feature를 3개(x,x&lt;em&gt;x,x&lt;/em&gt;x*x) 사용하게 된다면, 3차 함수가 되어서 , 위와 같은  model을 만들게 될 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/170037425-e9b1f125-6509-4e66-ae16-905a7445b020.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 , feature를 다르게 사용한다면(x,x&lt;em&gt;x x&lt;/em&gt;(1/2) (root x) ) 위와같이 점진적으로 증가하는 model이 되어서 데이터를 잘 표현하는 model이 될 것입니다.&lt;/p&gt;

&lt;p&gt;위의 데이터의 경우 , feature가 x1 ,x2… xn 만큼 주어졋다하더라도, 임의의 feature xk만을 사용하여 표현할 수 있고 , 이 feature의 지수를 어떻게 설정하느냐에 따라 data를 저 잘 표현할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;learning-polynomial-regression&quot;&gt;Learning Polynomial Regression&lt;/h2&gt;

&lt;p&gt;model을 설정하였으면 ,  model을 학습해야 합니다. Polynomial Regression model 역시 Gradient Descent를 이용하여 학습을 진행합니다.&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent-in-multiple-variables&quot;&gt;Gradient Descent in Multiple Variables&lt;/h3&gt;

&lt;p&gt;우선 기존의 linear regression에서의 Gradient Descent를 equation으로 적으면 아래와 같습니다.
$$
\begin{align}
\theta_{0} := \theta_{1} - \alpha * {\partial J(\theta) \over \partial\theta_{0}} &lt;br /&gt;
\theta_{1} :=  \theta_{1} - \alpha * {\partial J(\theta) \over \partial\theta_{1}}&lt;/p&gt;

&lt;p&gt;\end{align}
\(Polynomial Regression의 경우에는 variable의 개수가 늘어나므로 여러번 update를 해주면 됩니다.\)
\begin{align}
\theta_{0} &amp;amp;:= \theta_{1} - \alpha * {\partial J(\theta) \over \partial\theta_{0}} &lt;br /&gt;
\theta_{1} &amp;amp;:=  \theta_{1} - \alpha * {\partial J(\theta) \over \partial\theta_{1}} &lt;br /&gt;
\theta_{2} &amp;amp;:=  \theta_{2} - \alpha * {\partial J(\theta) \over \partial\theta_{2}} &lt;br /&gt;
\theta_{3} &amp;amp;:=  \theta_{3} - \alpha * {\partial J(\theta) \over \partial\theta_{3}} &lt;br /&gt;
&amp;amp;…&lt;/p&gt;

&lt;p&gt;\end{align}
\(이를 일반화시키면 아래와 같은 eqation을 얻을 수 있습니다.\)
\begin{align}&lt;/p&gt;

&lt;p&gt;\theta_{k} &amp;amp;:=  \theta_{k} - \alpha * {\partial J(\theta) \over \partial\theta_{k}} \&lt;/p&gt;

&lt;p&gt;(k &amp;amp;= 1 … n)&lt;/p&gt;

&lt;p&gt;\end{align}
$$&lt;/p&gt;

&lt;h3 id=&quot;convergence-speed&quot;&gt;Convergence Speed&lt;/h3&gt;

&lt;p&gt;Polynomial Regression model을 학습할 방법을 알아봤습니다. 바로 학습에 들어가도 되지만, model 빠르게 convergence할수록 model을 학습하는 시간을 줄일 수 있습니다. 여러 hyperparameter들에 의해서 이 model이 convergence하는 시점이 빨라질 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;scaling-variables&quot;&gt;Scaling Variables&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/170268003-1fd87952-68ed-492f-9ba4-e0169e7117ae.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Variable간의 scale 차이가 크다면 Loss Function이  위와 같은 형태를 보일 것입니다.  scale이 큰 variable의  parameter를 update하면 loss function의 값이 상대적으로 크게 변하고 , scale이 작은 variable의 parameter를 update하면 loss function의 값이 상대적으로 작게  변하기 때문에 위와같이 찌그러진 모양이 나오게 됩니다.&lt;/p&gt;

&lt;h4 id=&quot;learning-rate&quot;&gt;Learning Rate&lt;/h4&gt;

\[\begin{align}

\theta_{k} &amp;amp;:=  \theta_{k} - \alpha * {\partial J(\theta) \over \partial\theta_{k}} \\

(k &amp;amp;= 1 ... n)

\end{align}\]

&lt;p&gt;Model의 weight를 update할 때  , \(\alpha\) 라는 term을 곱하게 됩니다. 이 값을 learning rate라고 합니다. 만약에  , 이 learning rate값이 크다면,  loss function의 값이 아래와 같이 convergence 할 것입니다. convergence 값이 일정하게 감소하지 않을 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/171631540-d4abe530-7f8d-4327-bc79-462b28a0f437.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약에 , learning rate 값이 작다면 , loss function의 값이 convergence하는데 시간이 오래 걸릴 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/171632233-0ca16407-0ddd-4db1-903e-ea386539c8d6.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;learning rate값이 적절한 값을 가진다면, loss function의 값은 상대적으로 잘 convergence할 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/171632544-af33dc99-875b-4b16-899f-62bbd3e2da6f.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;즉, 적절한 learning rate 값이란, 매 update마다 loss function의 값이 감소하면서 , 너무 적지 않게 감소하게 하는 값이라 할 수 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;p&gt;Standford-ml &lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot;&gt;기계 학습  Coursera&lt;/a&gt;&lt;/p&gt;</content><author><name>Choi Woongjoon</name></author><category term="StandfordML" /><category term="Machine Learning" /><category term="LinearRegression" /><category term="StandfordML" /><summary type="html">Standford ML</summary></entry><entry><title type="html">What is Objects and Why use Built-in objects</title><link href="http://localhost:4000/python/Built-in-Objects/" rel="alternate" type="text/html" title="What is Objects and Why use Built-in objects" /><published>2022-05-26T00:00:00+09:00</published><updated>2022-05-26T00:00:00+09:00</updated><id>http://localhost:4000/python/Built-in-Objects</id><content type="html" xml:base="http://localhost:4000/python/Built-in-Objects/">&lt;h1 id=&quot;what-is-objects&quot;&gt;What is Objects?&lt;/h1&gt;

&lt;p&gt;Python을 이용하여 Program을 만들려면 Object를 이용하여 Program을 만들게 됩니다. Object를 이용해서 Program을 만드는 것은 당연하다고 볼 수 있지만, 왜 굳이 Built-in Type을 사용해야 될까요?&lt;/p&gt;

&lt;h2 id=&quot;pythons-conceptual-hierachy&quot;&gt;Python’s Conceptual Hierachy&lt;/h2&gt;

&lt;p&gt;Python의 Object-Type을 이해할려면 Python Syntax가 어떤 계층으로 이루어져 있는지를 알아야 합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Program은 여러개의 Modules로 이루어져 있습니다.&lt;/li&gt;
  &lt;li&gt;Modules는 여러개의 Statements로 이루어져 있습니다.&lt;/li&gt;
  &lt;li&gt;Statement는 여러개의 Expression으로 이루어져 있습니다.&lt;/li&gt;
  &lt;li&gt;Expression은 Object-type을 process하거나 create 합니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;즉, 연역법에 의해서 , Python의 Program은 Object-type을 통해서만 작동됨을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;다시 말하면 , Python의 Program은 Object가 어떻게 작동하는지 알아야 , Python Program을 의도하는대로 작동시킬 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;why-use-built-in-types&quot;&gt;Why Use Built-in types?&lt;/h2&gt;

&lt;p&gt;Object를 잘 알아야하는 이유는 알겠는데 , 왜 굳이 Built-in Object Type을 사용할까요?  내가 필요한 Operation을 지원해주는 Object-Type을 사용하면 안될까요?&lt;/p&gt;

&lt;h3 id=&quot;cc--vs-python&quot;&gt;C++,C  vs Python&lt;/h3&gt;

&lt;p&gt;C++,C 에서하는 일을 생각해보면 Built-in type을 사용하는 이유를 알 수 있습니다. C,C++ 에서 주로 Object를 Implementation 하는데 초점이 맞추어져 있습니다. 이 object를 위해서 memory structure를 배치하고 ,  memory allocation을 관리하고  , 또한 search 와 access 루틴을 구현해야 합니다.&lt;/p&gt;

&lt;p&gt;하지만, Python에서는 Built-in Object를 사용한다면, C++,C에서 해야만 하는 작업들을 할 필요가 없습니다.&lt;/p&gt;

&lt;h3 id=&quot;other-reasons&quot;&gt;Other Reasons&lt;/h3&gt;

&lt;p&gt;위 이유 이외에도 다양한 여러가지 이유가 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;간단한 task의 경우 Built-in Object type이 내가 지금 풀고있는 problem domain에 필요로하는 data structure를 제공해줄 수 있습니다.&lt;/li&gt;
  &lt;li&gt;복잡한 task의 경우 , custom object나 c언어 interface를 사용해야 할 수 있습니다 .Built-in type objects는 custom object을 구성하는 요소입니다.&lt;/li&gt;
  &lt;li&gt;Built-in object는 custom data structure보다 효율적입니다. Python의 Built-in Object는 speed를 최적화하기 위해 C로 구현된 data structure를 사용합니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;pythons-core-data-type&quot;&gt;Python’s Core Data Type&lt;/h2&gt;

&lt;p&gt;Python의 Core Data Type은 아래와 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Object&lt;/li&gt;
  &lt;li&gt;Numbers&lt;/li&gt;
  &lt;li&gt;Strings&lt;/li&gt;
  &lt;li&gt;Lists&lt;/li&gt;
  &lt;li&gt;Dictionaries&lt;/li&gt;
  &lt;li&gt;Files&lt;/li&gt;
  &lt;li&gt;Sets&lt;/li&gt;
  &lt;li&gt;Other Core Types&lt;/li&gt;
  &lt;li&gt;Program Unit Types&lt;/li&gt;
  &lt;li&gt;Implementation-related data types&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;p&gt;Learning Python(by Mark Lutz)  - chp 4&lt;/p&gt;</content><author><name>Choi Woongjoon</name></author><category term="Python" /><category term="Python" /><category term="Object" /><category term="Builtin" /><summary type="html">Python Objects</summary></entry><entry><title type="html">Setting Your Goal</title><link href="http://localhost:4000/dls_c3/Setting-your-goal/" rel="alternate" type="text/html" title="Setting Your Goal" /><published>2022-05-25T00:00:00+09:00</published><updated>2022-05-25T00:00:00+09:00</updated><id>http://localhost:4000/dls_c3/Setting-your-goal</id><content type="html" xml:base="http://localhost:4000/dls_c3/Setting-your-goal/">&lt;h1 id=&quot;setting-your-goal&quot;&gt;Setting your Goal&lt;/h1&gt;

&lt;p&gt;머신러닝 모델을 훈련을 시킬때 , 한가지 모델만을 훈련시키고 끝나는 것이 아니라 여러가지의 모델을 훈련시킨후 이 중에서 가장 잘 작동할 법한 모델을 선택하게 됩니다. 이 때 , 어떠한 기준치로 모델을 선택하는데 , 이 기준치를 Metric 이라 합니다. 이 Metric은 정렬해서 가장 좋은 모델을 고르는건 어렵지 않지만, Metric을 어떻게 설정하는지 모른다면 좋은 모델을 고를 수 없습니다.&lt;/p&gt;

&lt;h2 id=&quot;metric-is-case-by-case&quot;&gt;Metric is case by case&lt;/h2&gt;

&lt;p&gt;Metric을 설정하는 것은 case by case 라는 것을 기억해야 합니다. Metric은 Machine Learning Project Team이 어떤 Goal을 지향하는지에 따라서 달라집니다. 어떤 Team은 low latency가 가장 중요할 수 도 있고 , 여러개의 지표를 한번에 고려할 수 도 있습니다. Team에서 회의를 통해 어떤 Metric을 중요시 할 지 결정하게 될 것입니다. 이를 가지고 , 어떻게 Metric을 잘 설정하면 좋을지 이에 대해 간단한 방법을 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;using-single-number-metric&quot;&gt;Using Single Number Metric&lt;/h2&gt;

&lt;p&gt;여러 Metric을 전부 고려해야하는 경우 , 이를 한가지의 Number로 합칠 수 있는 경우가 있습니다. 예를들어 , Recall, Precision은 Model이 주어진 DataSet에 대하여 얼마나 좋은 performance를 보여주는지에 대한 Metric입니다. 이러한 Metric에 대한 평균값을 사용하여 , 좋은 값을 찾을 수 있습니다. 평균중에서도 산술평균, 기하평균,조화평균 을 사용하거나, Metric을 Parameter로 사용하는 새로운 function을 만들 수 도 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;satisfying-conditions-and-optimizing&quot;&gt;Satisfying Conditions and Optimizing&lt;/h2&gt;

&lt;p&gt;위와 같은 경우가 아닌 , latency와 precision을 동시에 고려해야 하는 경우가 있다면 , 이 두 metric을 합치는 것은 어색합니다. 따라서 , 이러한 경우에는 제일 우선시하는 metric를 선정하고 나머지 metric이 만족해야하는 조건을 설정하는 방법이 있습니다. 보통 , 나머지 metric들은 어느 조건을 만족하게 되면 , 그 이상을 잘하더라도 신경쓰지 않는 경우가 많습니다 . N개의 metric이 있다고 가정을 한다면 , N-1개의 metric이 조건을 만족한다면 나머지 1개의 metric으로 best model을 선정할 수 있습니다 .&lt;/p&gt;

&lt;p&gt;예를 들면 , 만족해야 하는 metric이 latency , precision , user response time , memory size 라고 하겠습니다. 가장 중요시 되는 metric이 precision이라 설정하면 , latency는 100ms 이하 , user response time은 150ms 이하 memory size는 1gb 이하등으로 설정할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;train--dev--test&quot;&gt;Train / Dev / Test&lt;/h2&gt;

&lt;h3 id=&quot;dev-distribution-vs-test-distribution&quot;&gt;Dev Distribution vs Test Distribution&lt;/h3&gt;

&lt;p&gt;Data를 수집하게 되면, Train , Dev , Test로 나누게 됩니다 . Train data는 model의 weight를 update하는데 사용되어 지는 data이고 , dev data는 훈련된 model이 unseen data에서 얼마나 잘하는지를 평가하는 data이고 , test data는 평가한 model이 실제 real world의 data가 주어질 때 얼마나 잘할지를 측정하는 data입니다 .&lt;/p&gt;

&lt;p&gt;data를 3개의 category로 나누게 될 때 , dev,test의 분포가 다르게 되면 model이 real world에서 예상 performance와는 다른 performance를 보여줄 수 있습니다 .&lt;/p&gt;

&lt;p&gt;예를 들어 , 글로벌 얼굴 인식 모델을 만든다고 가정하겠습니다. dev에는 asia 사람들의 data, africa 사람들의 data , test에는 europe ,america등 기타지역 사람들의 data를 사용하기로 결정했습니다. 만일 이렇게 된다면 ,      asai, africa 사람들에 대해서 잘 작동하는 model이 될 것입니다 . 따라서, 여기서 이상적인 dev,test dataset의 설정법은 모든 지역의 data를 shuffle한뒤 dev , test 로 나눈다면 , dev set에서의 좋은 performance가 test에서의 좋은 performance로 연결될 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;train--dev--test-split&quot;&gt;Train , Dev , Test Split&lt;/h3&gt;

&lt;p&gt;DataSet을 수집하고 , Train , Dev, Test 를 나누게 되는데 , Test Set을 두지 않을 경우 , Train,Dev의 비율을 7:3으로 설정하고 , Test Set이 있을경우에는, Train,Dev,Test의 비율을 6:2:2 로 설정하는 것이 rule of thumb(관례적으로 좋음)이라고 합니다.&lt;/p&gt;

&lt;p&gt;하지만 , 위의 얘기는 big data로 deep learning model을 학습하기 이전의 얘기입니다. 예전에는 , 10000개 정도의 수준이였지만, big data시대에서는 data가 100만개를 넘어가는 경우가 빈번합니다.  통상적으로 , data가 100만개를 넘어가는 경우에는 dev set, test set을 20만개정도로 설정할 필요가 없습니다. 만개정도의 test set이랑 dev set이 있으면 괜찮습니다.  즉 , dev set과 test set의 비율이 중요한것이 아니라 evaluation 과정을 진행할 수 있을 정도의 data set이 있으면 충분합니다.&lt;/p&gt;</content><author><name>Choi Woongjoon</name></author><category term="DLS_C3" /><category term="Machine Learning" /><category term="Coursera" /><category term="deep learning" /><summary type="html">DeepLearning.ai Dls ML strategy (Metric, Data Split)</summary></entry><entry><title type="html">Residual Network : intuition</title><link href="http://localhost:4000/dlarchitecture/ResNet/" rel="alternate" type="text/html" title="Residual Network : intuition" /><published>2022-03-26T00:00:00+09:00</published><updated>2022-03-26T00:00:00+09:00</updated><id>http://localhost:4000/dlarchitecture/ResNet</id><content type="html" xml:base="http://localhost:4000/dlarchitecture/ResNet/">&lt;p&gt;이론적으로 NeuralNet의 layer가 증가하면, Parameter의 수가 증가하므로 , training에서의 performance가 증가해야합니다. 하지만, 실제로는 training error가 줄어들다가 어느 순간부터 증가합니다. 이에 대해서, 알아보고자 합니다.&lt;/p&gt;

&lt;h1 id=&quot;residual-block&quot;&gt;Residual Block&lt;/h1&gt;

&lt;p&gt;Deep Neural Net에서 위와 같은 문제점이 발생하는 이유는 gradient가 폭발적으로 증가하거나, 0에 가깝게 감소하기 때문입니다.&lt;/p&gt;

&lt;p&gt;이를, gradient exploding ,gradient vanishing이라 부릅니다. 이를 해결하기 위한 architecture로 ResNet이 있습니다.&lt;/p&gt;

&lt;p&gt;Neural Net의 Layer Block은 아래와 같이 이루어 집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/160216325-cd462626-a284-4917-83d4-17c447a34b64.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;NeuralNet 의 layer , activation을 forward propagation을 시킨 것을 a 라 하겠습니다. a[L]은  l번째 layer의 forward propagtion후 activation을 적용한 output이라 정의 하겠습니다. l번째 layer에서 forward propagation을 한 것을 z[L] 이라 하겠습니다. 마찬가지로, weight는 W[L] , bias는 b[L] , activation 은 g[L] 이라 정의하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/50165842/160216684-69e00ecb-4693-4415-90e7-4fe054d94fd6.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Residual Block이란 a[L+2] 를 계산하기 전에 a[L]을    z[L+2] 을 concatenate한 다음에 , a[L+2]를 계산하는 것입니다.&lt;/p&gt;

\[\begin{align}
z^{[L+1]} &amp;amp;= W^{[L+1]}a^{[L]} + b^{[L+1]} \\
a^{[L+1]} &amp;amp;= g^{[L+1]}(z^{[L+1]}) \\
z^{[L+2]} &amp;amp;= W^{[L+2]}a^{[L+1]} + b^{[L+2]} \\
a^{[L+2]} &amp;amp;= g^{[L+2]}(z^{[L+2]} +a^{[L]} ) \\
\end{align}\]

&lt;p&gt;수식으로 표현하게 되면 위와 같이 표현할 수 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;why-resnet-work&quot;&gt;Why Resnet Work?&lt;/h1&gt;

&lt;p&gt;ResNet 은 어떤 원리로 작동하게 되는 것일까요? 즉 , ResNet이 어떤 원리로 gradient vanishing이나 gradient exploding을 해결할까요?
\(a^{[L+2]} = g^{[L+2]}(W^{[L+2]}a^{[L+1]} + b^{[L+2]} + a^{[L]})\)
L+2 layer의 activation을 풀어서 전개하게 되면 , 위와 같이 전개할 수 있습니다. 만약에 , W, b 가 0라고 가정을 하면, 
\(a^{[L+2]} = g^{[L+2]}( a^{[L]}) = a^{[L]}\)
위와 같이 식이 단순화됩니다. Activation Function을 ReLU를 사용하게 된다면,  l번째 layer의 activation이 0 이상이기 때문에 , l번째 layer의 activation이 결과값이 됩니다. 따라서,  forward propagation시 L+2 번째 layer의 activation은 그저 항등함수가 될 것이고 , 이는 다른 복잡한 function 보다는 비교적으로  배우기가 쉽습니다.&lt;/p&gt;

&lt;p&gt;그렇다면, 위의 과정에서 L+2번째 layer의 affine transformation이 무시된다면, hidden layer가 반드시 필요한 것일까라는 의문이 들 수 있습니다.&lt;/p&gt;

&lt;p&gt;단순히 hidden-layer없이 , activation만이 전달되는 상황을 가정한다면, 단순히 항등함수만을 배우게 될 것입니다. 하지만, hidden-layer가 존재한다면 좀 더 복잡한 function을 배울 수 있습니다. 따라서, hidden layer를 추가하게 되면, Identity function으로 activation이 그대로 전달되어 최소한의 성능이 보장이 되고 , 뿐만 아니라, 좀더 복잡한 function으로 성능이 더 올라갈 수 있습니다. 왜냐하면 , 복잡한 function을 만들게 되면 , 이에 대한 activation이 L+2 번째 activation에 반영이 되고 , back-propagation으로 배울수 있기 때문입니다. L+2번째  layer의 activation이 0보다 크더라도 , back-propagation시 L번째 layer 에 대한 gradient가 직접적으로 전달되기 때문에 , vanishing gradient나 exploding gradient가 발생하지 않습니다.&lt;/p&gt;

&lt;p&gt;따라서, hidden layer를 추가하는게 좋은 선택입니다.&lt;/p&gt;

&lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt;

&lt;p&gt;VGG architecture의 등장이후 ,Linear,Relu Block을 2번 정도 거치면 channel수를 2배정도로 늘리는 architecture가 보편화 되었습니다. ResNet에서도 이것이 적용되었습니다.
\(a^{[L+2]} = g^{[L+2]}(z^{[L+2]} + W_{s}a^{[L]})\)
즉 , L+2번째 layer의 channel수와 L번째 layer의 channel수가 다릅니다. 예를 들어, L+2번째의 channel수가 256이라면, l번째 layer는 128정도가 될 것입니다. 따라서, matrix를 통해서 dimension을 변화시켜주어야 합니다. 
\(a^{[L+2]} \in R^{n^{[L+2]}   \times m} \qquad a^{[L]} \in R^{n^{[L]} \times m } \qquad W_s \in R^{n^{[L+2]} \times n^{[L]} }\)
단순하게 , Linear Layer라 생각하면 dimension을 위와 같이 설정할 수 있을 것입니다.&lt;/p&gt;</content><author><name>Choi Woongjoon</name></author><category term="DLArchitecture" /><category term="Machine Learning" /><category term="ResNet" /><category term="deep learning" /><summary type="html">ResNet Intuition</summary></entry><entry><title type="html">Python loop statement general format and general definition</title><link href="http://localhost:4000/python/Python-loop-statements-general-format/" rel="alternate" type="text/html" title="Python loop statement general format and general definition" /><published>2022-03-15T00:00:00+09:00</published><updated>2022-03-15T00:00:00+09:00</updated><id>http://localhost:4000/python/Python-loop-statements-general-format</id><content type="html" xml:base="http://localhost:4000/python/Python-loop-statements-general-format/">&lt;h1 id=&quot;python-loop-statements-general-format&quot;&gt;Python loop statements general format&lt;/h1&gt;

&lt;p&gt;Python에서 사용할 수 있는  loop statement로는 2가지가 있습니다. 하나는, for statement, 다른 하나는 while statement입니다. 어떻게 사용하는지는 다들 대략적으로 알고 있습니다.&lt;/p&gt;

&lt;p&gt;하지만, 이에 대한 semantic한 definition 과 general 한 syntax에 대해서는 잘 알지 못할것입니다. 이렇게 , 공부하는 것은 Programming language를 배울때 안좋다고 하지만, 저는 formulate하는 것이 이러한 logic을 처음 보는 문제에 응용하는 좋은 방법이라고 믿고 있습니다. for statment, while statement, loop 에서만 허용된 statement에 대해서 semantic 한 definition과 general 한 syntax로 나타내보도록 하겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;while&quot;&gt;while&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;while statement는 general 한 loop 를 나타냅니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉, general한 proecedure 를 만들기 위한 statement입니다. while loop로는 모든 종류의 loop를 만들수 있습니다.&lt;/p&gt;

&lt;p&gt;while의 general한 format은 아래와 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while test : # test expression
	statements # intended expression
	if test : continue  # Go to top of loop
	if test : break  #Exit loop
[else :] # Optional
	statements
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;for&quot;&gt;for&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;for loop은 iterable objects나 sequence 안에 있는 item들을 조회해나가는 statement 입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;iterable objects에는 list,tuples, sets, user-defiend iterable objects등이 포함될 수 있습니다.&lt;/p&gt;

&lt;p&gt;for statement의 general한 format은 아래와 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for target in iterable :  # Assign item in iterable to target
	statements
	if test : continue  # Go to top of loop
	if test : break  #Exit loop
[else :] # Optional
	statements
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;target-assignment&quot;&gt;target assignment&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;target in iterable&lt;/code&gt;  은 assignment statement라 봐도 괜찮습니다. 즉 , iterable 안의 item이 target에 assign 됩니다.&lt;/p&gt;

&lt;p&gt;assignment이면서 어떤 종류의 assignent도 제한이 없습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for (a,b) in S :
	print(a,b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;tuple 형태의 assignment도 가능합니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for target in S :
	a,b = target
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이와 같이 sequence를 unpack할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;뿐만 아니라 , target에 어떤형태의 expression도 가능하기에 , nested expression도 가능합니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for ((a,b),c) in S :
	print(a,b,c)
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for (a, *b, c) in S :
	print(a,b,c)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;starred expression도 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;즉 , Python assignment에서 허용된 syntax라면 전부 사용할 수 있습니다.&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;continue-break-loop-else&quot;&gt;Continue, break, loop else&lt;/h1&gt;

&lt;h2 id=&quot;continue--break&quot;&gt;continue , break&lt;/h2&gt;

&lt;p&gt;continue, break는 자주 쓰이지만, 이에 대해서 define해보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;continue statement는 즉각적으로 바로 loop의 맨 위로 가게 하는 statement입니다.&lt;/p&gt;

&lt;p&gt;break statement는 바로 loop를 나오게 하는 statement 입니다.&lt;/p&gt;

&lt;h2 id=&quot;loop-else&quot;&gt;loop else&lt;/h2&gt;

&lt;p&gt;loop else는 loop statement의 test가 False가 되면 실행이됩니다.  혹은, for loop에서 iterable이 소진이 되면 사용될 수 있습니다. 하지만, break statement로 loop가 종료가 되면 실행되지 않습니다.&lt;/p&gt;

&lt;p&gt;이는 try except statement와 비슷한데, try statament에서 else는 except가 실행되지 않으면 실행되고 , for loop or while loop에서는 break가 실행되지 않으면 else가 실행됩니다.&lt;/p&gt;

&lt;p&gt;loop 에서 flag status를 이용하여 loop를 실행할 때 , 원하는 조건을 만족하지 못하고 전체 loop를 iterate하는 경우, flag를 이용한 coding structure를 자주 사용하게 됩니다. 하지만, loop else를 사용하면 좀더 explicit한 coding structure를 제공해준다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;즉, flag status 나 다른 condition을 설정하지 않고 , loop flow에서 벗어나는 flow를 control 할 수 있게 해줍니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for n in range(2, 10):
...     for x in range(2, n):
...         if n % x == 0:
...             print(n, 'equals', x, '*', n//x)
...             break
...     else:
...         # loop fell through without finding a factor
...         print(n, 'is a prime number')
...
2 is a prime number
3 is a prime number
4 equals 2 * 2
5 is a prime number
6 equals 2 * 3
7 is a prime number
8 equals 2 * 4
9 equals 3 * 3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;소수를 구하는 예제인데 , factor를 발견하지 못한다면, loop else를 사용하여 소수임을 알려줍니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for n in range(2, 10):
		flag =False
...     for x in range(2, n):
...         if n % x == 0:
...             print(n, 'equals', x, '*', n//x)
				flag = True
...             break
...     if not flag  :
			print(n, 'is a prime number')
...
2 is a prime number
3 is a prime number
4 equals 2 * 2
5 is a prime number
6 equals 2 * 3
7 is a prime number
8 equals 2 * 4
9 equals 3 * 3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;만약에 , loop else가 없다면 flag status를 이용하여 자연스럽지 않은 coding structure를 사용하게 될 것입니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while x: # Exit when x empty
     if match(x[0]):
         print('Ni')
         break # Exit, go around else
     x = x[1:]
else:
     print('Not found') # Only here if exhausted x
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;while의 loop를 통해 발견하지 못한다면 loop else 로 그 flow를 처리해줄 것입니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;found = False
while x and not found:
     if match(x[0]): # Value at front?
         print('Ni')
         found = True
     else:
         x = x[1:] # Slice off front and repeat
if not found:
     print('not found')	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;loop else를 사용하지 않는다면 flag status로 따로 loop 밖에서 발견하지 못한  flow를 처리해 주어야 할 것입니다.&lt;/p&gt;</content><author><name>Choi Woongjoon</name></author><category term="Python" /><category term="Python" /><category term="loop" /><category term="Statemens" /><summary type="html">for and while detail in Python</summary></entry></feed>